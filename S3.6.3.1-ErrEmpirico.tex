%
\subsubsection{Derivación de la función error empírico}
%
La función de error empírico se basa en el error del modelo sobre un
conjunto de prueba de clase conocida, el cual es un estimador no
sesgado del error de generalización.  El \e{error de prueba} del
modelo $h(\xx)$ sobre el conjunto
$T=((\tilde{x}_k,\tilde{y}_k)),\,k=1,\ldots,N$ viene dado por
la \iflatexml{}Ecuación~\ref{eq:error-prueba}\else\autoref{eq:error-prueba}\fi,
y puede escribirse en la forma equivalente
%
\begin{align}
\label{e3:error-test-alt}
  E^T &= \frac{1}{N}\sum_{k=1}^{N} H(-{y}_k {c}_k)
\end{align}
%
donde $c_k=h(\xx_k)$ es la salida del modelo, y
$H(\cdot)$ es la función escalón de Heaviside. Fijando el núcleo
utilizado y los conjuntos de datos de entrenamiento $D$ y prueba $T$, se
puede considerar
%
\begin{align}
\label{e3:error-test-func-theta}  
  E^T = E^T(\Btheta),
\end{align}
%
implicando que el el error del modelo sobre el conjunto $T$ variará
para diferentes valores del vector de hiperparámetros $\Btheta$.
Desde este punto de vista, minimizar $E^T(\Btheta)$ equivale a
seleccionar los hiperparámetros óptimos $\Btheta$ para los conjuntos
$D$ y $T$ y el tipo de núcleo fijados.

Si bien la función $E^T$ es una función objetivo válida para la
comparación de modelos, por definición es una función discontinua, lo
que imposibilita la optimización eficiente mediante descenso de
gradiente. Una interpretación alternativa del concepto de error,
basada en la teoría bayesiana, permite desarrollar una función
objetivo con características deseables de continuidad y derivabilidad.

Si se conoce la \e{probabilidad a posteriori} $p_k$ de que el ejemplo
$\xx_k$ pertenezca a la clase positiva
%
\begin{align}
  \label{e3:pk}
  p_k = p(x_k) = P(c_k=+1|x_k),
\end{align}
%
se puede caracterizar la \e{probabilidad de error} $E_k$ cometido al
clasificar el ejemplo $\xx_k$ según
%
\begin{align}
\label{e3:Ek}
  E_k = P(c_k\neq y_k) = |t_k-{p}_k| =
  \begin{cases}
    {p}_k, & t_k=0\\ 1-{p}_k, & t_k = 1,
  \end{cases}
\end{align}
%
donde $t_k=\frac{y_k+1}{2}$ es un ``valor deseado'' calculado a partir
de la clase conocida $y_k$. La probabilidad de error para el conjunto
completo $T$ puede escribirse
%
\begin{align}
\label{Err1}
  E = \sum_{k=1}^{N} E_k.
\end{align}
%
La función $E$ es el \e{error empírico} y es la función objetivo a
minimizar. El cálculo de $E$ requiere el conocimiento de la
probabilidad $p_k$ (\iflatexml{}Ecuación~\ref{e3:pk}\else\autoref{e3:pk}\fi),
que no puede determinarse directamente a partir de la salida binaria
del modelo SVM. En cambio, se utiliza un estimador de la probabilidad
$\hat{p}_k$, tal como se detalla a continuación.
