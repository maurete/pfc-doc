%
\subsubsection{Criterio de corte}
%
En general, la convergencia del algoritmo de retropropagación no está
garantizada, y no existe un criterio de corte bien definido para
detener el entrenamiento. Algunos criterios razonables para establecer
la convergencia pueden ser:
%
\begin{itemize}
\item Cuando la norma del gradiente es muy pequeña
  ($\|\nabla\C{E}\|\leq\epsilon$), probablemente se deba a que se
  alcanzó un mínimo local de la función de error.
\item Cuando la función de energía promedio $\C{E}_{\T{av}}$ varía muy
  poco entre épocas, también podría significar que se alcanzó un
  mínimo local, indicando convergencia.
\end{itemize}
%
El problema de estos criterios es similar al de la velocidad de
aprendizaje: los valores de tolerancia a considerar para la
convergencia serán siempre dependientes del problema, y una elección
incorrecta puede llevar ya sea a un corte prematuro del entrenamiento,
o bien a que el algoritmo nunca converja.

Existe sin embargo un criterio de convergencia que tiene a la vez un
fundamento teórico y es independiente del conjunto de datos de
entrenamiento: luego de cada iteración de entrenamiento, se clasifica
un conjunto de prueba $T$
(\iflatexml{}Ecuación~\ref{eq:conj-prueba}\else\autoref{eq:conj-prueba}\fi),
obteniendo una estimación de error de generalización, y se detiene el
entrenamiento cuando se observa que este error es adecuado, o bien
cuando resulta evidente que ha alcanzado su mínimo. Esta estrategia es
la que se utiliza prácticamente en todas las implementaciones: incluso
cuando no se provee un conjunto de prueba separado, resulta común
reservar parte del conjunto de entrenamiento para estimar el error de
generalización.
