%
\subsubsection{Cálculo del gradiente $\grad{E}$}
\label{se:gradE}
%
Considerando un vector genérico de \hparam{s}
$\Btheta=(\theta_1,\theta_2,\ldots)$, el gradiente
$\nabla{}E=\left(\dpar{E}{\theta}{}\right)$ viene dado por
%
\begin{align}
\label{gradE}
  \dpar{E}{\theta_j}{} = \sum_{k=1}^{\ell^V} \dpar{E_k}{\theta_j}{} =
  \sum_{ \{m:t_k=0\}  } \dpar{\hat{p}_m}{\theta_j}{}
  - \sum_{ \{n:t_k=1\}  } \dpar{\hat{p}_n}{\theta_j}{}
  = \sum_{k=1}^{\ell^V} y_k \dpar{\hat{p}_k}{\theta_j}{}.
\end{align}
%
Aplicando la regla de la cadena y calculando la derivada
$\dpar{\hat{p}_k}{f_k}{}$, se tiene
%
\begin{align}
  \dpar{\hat{p}_k}{\theta_j}{} =
  \dpar{\hat{p}_k}{f_k}{}\dpar{f_k}{\theta_j}{} =
  -A\hat{p}_k(1-\hat{p}_k)\dpar{f_k}{\theta_j}{}.
  \label{eq:deriv-pk-thetaj}
\end{align}
%
Para el cálculo de $\dpar{f_k}{\theta_j}{}$, se recurre a una
interpretación geométrica del modelo de la SVM con regularización $L1$
tal como en \cite{keerthi,glasmachers,shark}.
En primer lugar, se observa que $f_k$ viene dado por
%
\begin{align}
  f_k = \pint{\ww}{\Phi(\xx_k)}+b
      = \sum_{i=1}^\ell \alpha_i y_i k(\xx_i,\xx_k) + b,
  \label{fk}
\end{align}
%
en donde $((\xx_i,y_i)),\,i=1,\ldots,\ell$ son los ejemplos del
conjunto de entrenamiento, $k(\cdot,\cdot)$ es la función núcleo, y
$(\alpha_i, b)$ son los parámetros del modelo $h$.
Aplicando la regla de la derivada de un producto,
$\dpar{f_k}{\theta_j}{}$ viene dada por
%g
\begin{align}
  \dpar{f_k}{\theta_j}{} = \sum_{i=1}^\ell y_i
  \left[\dpar{\alpha_i}{\theta_j}{} k(\xx_i,\xx_k) + \alpha_i
    \dpar{k(\xx_i,\xx_k)}{\theta_j}{} \right].
\end{align}
%
Según sea el valor del multiplicador $\alpha_i$, correspondiente al
vector de entrenamiento $\xx_i$, se definen los conjuntos de índices
%
\begin{align}
  \label{unbounded-sv-set}
  u &= \left\{i\in\{1,\ldots,\ell\}:0<y_i\alpha_i<C \right\}\\
  \label{bounded-sv-set}
  g &= \left\{i\in\{1,\ldots,\ell\}: y_i\alpha_i=C \right\}\\
  n &= \left\{i\in\{1,\ldots,\ell\}: \alpha_i=0 \right\}.
\end{align}
%
Por interpretación geométrica de la solución al problema de la SVM
(\ref{svmprob-dual-soft}), se sabe que para aquellos vectores $\xx_i$
que no son de soporte, se cumple que $\alpha_i=0$, luego las derivadas
$\dpar{\alpha_n}{\theta_j}{}$ son nulas.
Cuando $\alpha_i=\pm{}C$, el valor de $\alpha_g$ viene limitado (en
valor absoluto) por el \hparam{} $C$, y las derivadas
$\dpar{\alpha_g}{\theta_j}{}$ son entonces
%
\begin{align}
  \dpar{\alpha_g}{C}{} \tab= y_g, \tabs
  \dpar{\alpha_g}{{\theta}^K_j}{} \tab= 0,
\end{align}
%
en donde $\theta_j^K$ es un hiperparámetro del núcleo.
Para simplificar el cálculo de las derivadas de $\alpha_u$ y $b$, se
plantea el problema en forma matricial.
Sea la \e{matriz del núcleo} $K$
%
\begin{align}
  K = \begin{pmatrix} k(\xx_1,\xx_1) & k(\xx_1,\xx_2) & \cdots & k(\xx_1,\xx_\ell)
    \\ k(\xx_2,\xx_1) & k(\xx_2,\xx_2) & \cdots & k(\xx_2,\xx_\ell) \\ \vdots &
    \vdots & \ddots & \vdots \\ k(\xx_\ell,\xx_1) & k(\xx_\ell,\xx_2) & \cdots &
    k(\xx_\ell,\xx_\ell)
  \end{pmatrix}
  =
  \begin{pmatrix}
    (K_{uu}) & (K_{ug}) & (K_{un}) \\
    (K_{gu}) & (K_{gg}) & (K_{gn}) \\
    (K_{nu}) & (K_{ng}) & (K_{nn})
  \end{pmatrix}.
\end{align}
%
En la matriz de la derecha, los elementos de $K$ han sido reordenados
en submatrices $K_{uu},K_{ug},\ldots$ según los conjuntos de índices
$u, g, n$ definidos anteriormente.
A partir de la sub-matriz $K_{uu}$ se define
%
\begin{align}
  H=\begin{pmatrix} K_{uu} & \B{1}_u \\ \B{1}_u^T & 0
  \end{pmatrix},
\end{align}
%
en donde $\B{1}_u$ es un vector columna de $|u|$ elementos iguales a 1.
La derivada de $(\B{\alpha}_u,b)^T$ respecto de un \hparam{}
$\theta_j$ viene dada por
%
\begin{align}
  \dpar{}{\theta^K_j}{} \begin{pmatrix}\alpha_{u}\\b\end{pmatrix} &=
    -H^{-1} \left[
      \dpar{H}{\theta^K_j}{}
      \begin{pmatrix}\alpha_{u}\\b\end{pmatrix}
        +C \begin{pmatrix}\dpar{K_{gu}}{\theta^K_j}{}\\0\end{pmatrix}
          y_g
          \right],
\end{align}
%
donde $\B{1}_g$ es un vector columna de $|g|$ elementos iguales a 1 y
$\B{y}_g$ es el vector de clases correspondientes a los elementos en
$g$.
Cuando $\theta_j = C$, se tiene
%
\begin{align}
  \dpar{}{C}{} \begin{pmatrix}\alpha_{u}\\b\end{pmatrix} &=
    -H^{-1} \left[
      \begin{pmatrix}{K_{gu}}\\\B{1_g^T}\end{pmatrix} y_g
      \right].
\end{align}
%
Para los detalles de este resultado, se refiere al lector a
\cite{glasmachers} y \cite{keerthi}.
Una vez conocidas las derivadas
$\dpar{({\B{\alpha},b})^T}{\theta_j}{}$, la derivada
$\dpar{f}{\theta_j}{}$ se calcula según
%
\begin{align}
  \dpar{f_k}{\theta^K_j}{} &=  \sum_{i=1}^l y_i \left[
    \dpar{\alpha_i}{\theta^K_j}{} k(x_i,x_k) +
    \dpar{k(x_i,x_k)}{\theta^K_j}{} \alpha_i \right]
  + \dpar{b}{\theta^K_j}{}, \\
    \dpar{f_k}{C}{} &=  \sum_{i=1}^l y_i\left[
    \dpar{\alpha_i}{C}{} k(x_i,x_k) \right]
  + \dpar{b}{C}{}.
\end{align}
%
Reemplazando estos resultados en (\ref{eq:deriv-pk-thetaj}), el
gradiente $\nabla{}E=\left(\dpar{E}{\theta_j}{}\right)$ (\ref{gradE})
se calcula directamente.
