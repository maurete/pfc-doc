%
\subsubsection{Rprop}
%
El algoritmo Rprop (\eng{Resilient backpropagation},
``retropropagación resiliente'') \cite{rprop} es una modificación
heurística sencilla de la ``regla delta'' que determina un valor de
actualización específico para cada peso $\Delta_{ij}$ mediante
%
\begin{align}
  \Delta{}w_{ij}(n) = 
  \begin{cases}
    -\Delta_{ij}^{(n)}, & \T{si } \dpar{E}{w_{ij}}{}(n) > 0 \\
    +\Delta_{ij}^{(n)}, & \T{si } \dpar{E}{w_{ij}}{}(n) < 0 \\
    0, & \T{en otro caso}.
  \end{cases}
\end{align}
%
Los valores $\Delta_{ij}$ se establecen inicialmente a un valor
$\Delta_0$, que es parámetro de Rprop, y se adaptan en
cada iteración según
%
\begin{align}
  \Delta{}_{ij}^{(t)} \tab = 
  \begin{cases}
    \eta^+\,\Delta_{ij}^{(n-1)}, & \T{si }\dpar{E}{w_{ij}}{}^{(n-1)}
      \dpar{E}{w_{ij}}{}^{(n)} > 0 \\
    \eta^-\,\Delta_{ij}^{(n-1)}, & \T{si }\dpar{E}{w_{ij}}{}^{(n-1)}
      \dpar{E}{w_{ij}}{}^{(n)} < 0 \\
    \Delta_{ij}^{(n-1)}, & \T{en otro caso},
  \end{cases}
  \tabs 0<\eta^-<1<\eta^+.
\end{align}
%
Adicionalmente, el valor de $\Delta_{ij}^{n}$ se limita en valor
absoluto $\Delta_{T{máx}}$, que es el segundo parámetro de Rprop.  Los
factores de incremento $\eta^+$ y decremento $\eta^-$ se fijan en
$\eta^+=1,2$ y $\eta^-=0.5$.

Contrastando con los algoritmos que ajustan la velocidad de
aprendizaje, Rprop modifica directamente el tamaño del paso
introduciendo el concepto de valores de actualización ``resilientes'',
logrando que el ``esfuerzo'' de adaptación no se ``desvíe'' con un
comportamiento impredecible del gradiente. De este modo, se logra un
entrenamiento estable, a expensas de un pequeño incremento en el
tiempo de entrenamiento, en comparación con otros algoritmos.

Dada la estabilidad del algoritmo, la elección de los parámetros
$\Delta_{0}$ y $\Delta_{T{máx}}$ no es crítica, por lo que en general
se utilizan directamente los valores por defecto $\Delta_{0}=0.1$ y
$\Delta_{T{máx}}=50$.
