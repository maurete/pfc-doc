%
\subsubsection{Parámetros e hiperparámetros del modelo}
%
En (\ref{e2:minerrorempirico}) se plantea el entrenamiento en términos
generales como la búsqueda del modelo óptimo ${h}$ sobre todos los
modelos posibles $\C{H}$.
En la práctica, la búsqueda se reduce a una familia de funciones
$\hat{\yy}=H(\xx,\B{p})$, y el objetivo del entrenamiento es encontrar
el valor óptimo $\B{p}^*$ que determina la función ${h}$
%
\begin{align}
  {h}(\xx) &= H(\xx,\B{p}^*), &
  \B{p}^*:=\arg \min_{\B{p}} \left\{ E_D(H(\,\cdot,\B{p})) \right\}.
  \label{e2:entrenamientoparams}
\end{align}
%
El modelo $\hat{\yy}=h(\xx)$ es entonces una instancia de $H$
con \e{parámetros} $\B{p}^*$.

En muchas máquinas de aprendizaje existe otro tipo de variables que
regulan el proceso de entrenamiento, y que incidien de manera indirecta
sobre el modelo generado.
En general estas variables se relacionan con propiedades tales como la
velocidad de convergencia del algoritmo, el grado de tolerancia a
errores o la complejidad del modelo resultante.
Para distinguirlas de los parámetros del modelo, se denomina a estas
variables como \e\hparam{s}.
El valor de los \hparam{s} debe establecerse \e{antes} de comenzar el
entrenamiento, según el conocimiento previo del problema.
Para algunas máquinas de aprendizaje, existen técnicas automáticas que
determinan los valores óptimos para los \hparam{s} a partir del
conjunto $D$, en una instancia previa al entrenamiento.
