%
\subsubsection{Retropropagación con término de momento}
%
%% Si la ``superficie'' de la función de error es ``chata'', una
%% velocidad de aprendizaje pequeña resultará en un tiempo de
%% convergencia muy elevado.  Similarmente, una velocidad de aprendizaje
%% grande posiblemente genere oscilaciones, previniendo que el error
%% caiga por debajo de un cierto valor.
%% Si bien la convergencia del algoritmo a un mínimo local puede probarse
%% en ocasiones, no existen garantías de encontrar el mínimo global de la
%% función de error.
Una estrategia simple que permite incrementar la velocidad de aprendizaje
evitando la inestabilidad es modificar la ``regla delta''
(\iflatexml{}Ecuación~\ref{e2:delta-rule}\else\autoref{e2:delta-rule}\fi)
añadiedo un \e{término de momento}
%
\begin{align}\label{e2:momentum-term}
  \Delta w_{ij}(n)\tab=-\eta\dpar{\C{E}(n)}{w_{ij}}{}
  +\mu\Delta w_{ij}(n-1).
\end{align}
%
El parámetro de momento $\mu$ controla el grado de influencia del paso
anterior en el paso actual: si el paso actual tiene el mismo sentido
que el anterior, el efecto del término de momento es aumentar la
velocidad de aprendizaje; si en cambio el paso actual es de sentido
opuesto al anterior, se reduce la velocidad de aprendizaje.

%% Esta técnica funciona bien en muchos problemas, aunque no significa un
%% incremento general de la estabilidad o de la velocidad de
%% convergencia. A veces, el descenso de gradiente sin término de momento
%% obtiene resultados iguales o mejores que con el término de momento.
%% Al utilizar aprendizaje con término de momento, probablemente se deba
%% disminuir la velocidad de aprendizaje para evitar inestabilidad en el
%% proceso de aprendizaje.
