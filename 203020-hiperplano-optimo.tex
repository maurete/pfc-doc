%
%
\subsection{El hiperplano de separación óptimo}
%
Dado un conjunto de entrenamiento
$D=\left((\xx_1,y_1),\ldots,(\xx_\ell,y_\ell)\right)$, se dice que
éste es linealmente separable en el espacio imagen $Z$ cuando existe
una función discriminante lineal $f(\xx)=\ww^T\BPhi(\xx)+b$ tal que su
signo permite determinar sin errores la clase de todos los elementos
del conjunto. Cuando el conjunto $D$ es linealmente separable en $Z$,
se sabe que existen infinitas funciones discriminantes lineales
capaces de clasificar correctamente todos los elementos.

La característica saliente de la máquina de vectores de soporte es
que, durante el entrenamiento, determina un hiperplano de separación
$\ww^T\zz+b=0$ \e{óptimo}, que maximiza la distancia a los puntos
$\zz_i=\BPhi(\xx_i)$ más cercanos. Esto se expresa a través del
problema de optimización
%
\begin{equation}
  \begin{aligned}
    \min_{\ww,b} \quad\tabs \frac{1}{2}\|\ww\|^2, \\
    \T{sujeto a}\quad\tabs y_i f_i\geq1,\,i=1,\ldots,\ell\,.
  \end{aligned}
  \label{e2:svm-problem-basic}
\end{equation}
%
Aquí, $f_i=f(\xx_i)=\ww^T\BPhi(\xx_i)+b$ es la función discriminante
lineal aplicada al vector $\xx_i$. En términos coloquiales, este
problema puede leerse
%
\begin{align*}
  \min \frac{1}{2}\|\ww\|^2 \quad \tabs \T{encontrar el $\ww$ más
    pequeño según la norma euclídea $\|\cdot\|$}\ldots\\
  |f_i|\geq1 \quad \tabs \T{estableciendo en $1$ la distancia del
    hiperplano a los puntos más cercanos},\\
  y_i\hat{y}_i>0\quad \tabs \T{y tal que la clase $y_i$ coincida con el
    signo del discriminante ($\hat{y}_i=\T{signo}(f_i)$)}.
\end{align*}
%
La distancia de un vector $\zz_i=\BPhi(\xx_i)$ al hiperplano
$\ww^T\zz+b=0$ viene dada por $d_i=\frac{\ww}{\|w\|}\cdot\zz_i+b$, por
ello, minimizar $\|w\|$ equivale a maximizar la distancia $d_i$.
Puede demostrarse que el vector solución $\ww_*$ al Problema
\ref{e2:svm-problem-basic} es una combinación lineal de aquellos
vectores $\zz_i$ para los cuales $|\ww^T\zz_i+b|=1$. Estos $\zz_i$ se
denominan \e{vectores de soporte}, lo que da el nombre al
clasificador.  Asimismo, los hiperplanos $\ww^T\zz+b=\pm1$ que
contienen los vectores de soporte conforman el llamado \e{margen} del
clasificador. El Problema \ref{e2:svm-problem-basic} también puede
entenderse como la maximización de la distancia del hiperplano
$\ww^T\zz+b=0$ al margen $\ww^T\zz+b=\pm1$. Esto da origen al nombre
alternativo de la máquina de vectores de soporte: \e{clasificador de
  margen máximo}.

%% Desde el punto de vista numérico, el problema
%% (\ref{e2:svm-problem-basic}) es un problema de optimización cuadrático
%% convexo con restricciones lineales.
