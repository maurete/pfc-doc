%
%
%
\section{Perceptrón multicapa}
%
%% El perceptrón multicapa (\eng{MultiLayer Perceptron}, MLP)
%% \cite{mlp2,mlp1} es una red neuronal \feedforward{}
%% (\eng{feedforward}), consistente en una serie de {unidades} o
%% neuronas organizadas en \e{capas} y conectadas por ``enlaces''
%% denominados \e{pesos sinápticos}.  Las capas se diferencian en
%% una capa \e{de entrada}, una o más capas \e{ocultas} y una capa
%% \e{de salida}.

%% La capa de entrada contiene nodos sensores que ``leen'' un vector
%% de entrada externo, y lo transmiten a través de los enlaces sinápticos
%% a la primer capa oculta. Las capas ocultas reciben como entrada las
%% salidas de la capa anterior, determinan su valor de salida aplicando
%% una \e{función de activación} a la suma ponderada sus entradas, y
%% transmiten su valor de salida a la capa siguiente. Este proceso
%% continúa capa por capa hasta alcanzar la capa de salida.

%% Desde un punto de vista global, un vector arbitrario es propagado
%% \e{hacia adelante} a través de la red, para finalmente obtener un
%% vector de activación en la capa de salida.  La función generada por la
%% red, que transforma un vector de entrada en uno de salida, se
%% determina completamente por los pesos sinápticos de todos los enlaces
%% de la red.  En la \refer{fig:mlp} se puede observar una representación
%% esquemática de un perceptrón multicapa con 3 entradas, 4 neuronas en
%% la capa oculta y 2 valores de salida.

%% --------------

El perceptrón multicapa (\eng{MultiLayer Perceptron}, MLP)
\cite{mlp2,mlp1} es una máquina de aprendizaje basada en una red
neuronal artificial consistente en una serie de {unidades} o neuronas
con una arquitectura organizadas en \e{capas}.


%% y conectadas por ``enlaces''
%% denominados \e{pesos sinápticos}
%% con una arquitectura organizada en capas.
%% Cada nodo se conecta mediante enlaces ponderados denominados
%% ``pesos sinápticos'' a todos los nodos en las capas adyacentes.

%% La primer capa, denominada capa de entrada, contiene nodos sensores
%% que ``leen'' un vector de entrada externo, y lo transmiten a través
%% de los enlaces sinápticos a la primer capa oculta.
%% Las capas subsiguientes se componen de ``neuronas'', nodos computadores
%% que determinan su valor de salida a partir de las salidas de los nodos en
%% la capa anterior y los valores de los pesos sinápticos que los conectan.

%% Las capas ocultas reciben como entrada las
%% salidas de la capa anterior, determinan su valor de salida aplicando
%% una \e{función de activación} a la suma ponderada sus entradas, y
%% transmiten su valor de salida a la capa siguiente. Este proceso
%% continúa capa por capa hasta alcanzar la capa de salida.

%% El MLP utiliza para su entrenamiento un algoritmo denominado de
%% ``retropropagación'', que entrena la red propagando una señal de error
%% a través de la misma, ajustando los ``pesos'' de las conexiones entre
%% neuronas a fin de lograr que la red efectúe una transformación deseada
%% de los datos de entrada leídos en la primer capa a valores de salida
%% apropiados en la última capa.

%% ----------- historia

%% - perceptrón simple, adaline, madaline
%% - retropropagación > mlp

%% ----------- neurona

%% - unidad computadora; suma ponderada (pesos), umbral, campo de activación
%% - función de activación: derivable, no lineal
%% - señales de error, ajuste pesos

%% ----------- arquitectura

%% - capas
%% - completamente conectadas
%% - propagacion del error

%% ----------- retropropagación

%% - propagacion del error, ajuste de los pesos
%% - derivadas
%% - variantes: rprop, scg

%
%
\subsection{Justificación: las redes neuronales biológicas}
%
Las redes neuronales naturales, entre las que contamos el cerebro
humano, son computadoras biológicas con características deseables que
no se encuentran en las computadoras clásicas, tales como un alto
grado de paralelismo, cómputo distribuido, capacidad de aprendizaje y
de generalización, adaptabilidad y tolerancia a fallos.  Las redes
neuronales artificiales (\eng{Artificial Neural Networks, ANNs}) son
un paradigma computacional inspirado en las redes neuronales
biológicas. Junto con un algoritmo de entrenamiento adecuado, las
ANNs pueden funcionar como máquinas de aprendizaje, al tiempo que
incorporan las características deseables de las redes naturales.

Biológicamente, una neurona (o célula nerviosa) es un tipo especial de
célula encargada de procesar información.  Se compone de un cuerpo
celular, denominado \e{soma}, y dos tipos de estructuras ramificadas:
el axón y las dendritas. Una neurona recibe señales (impulsos
eléctricos) provenientes de otras neuronas a través de sus dendritas
(receptores), y transmite señales generadas por su cuerpo celular a
través del axón, que se ramifica en ``hebras''.

Entre la terminal de una hebra proveniente de una neurona y la
dendrita de otra, se encuentra una estructura elemental denominada
sinapsis.  Cuando un impulso eléctrico alcanza la terminal de una
hebra, se liberan sustancias denominadas neurotransmisores. Estos
neurotransmisores se dispersan a través de la sinapsis, aumentando o
disminuyendo, dependiendo del tipo de sinapsis, la tendencia de la
neurona receptora de emitir un impulso eléctrico propio.

En el tiempo, la efectividad de las sinapsis se ajusta según las
señales que han pasado por ella, de modo que la propia sinapsis
\e{aprende} de las actividades en las que participa. Esta dependencia
de las señales históricas actúa como una \e{memoria}, y es
posiblemente la dimensión física de la memoria humana, relacionada
asimismo a la \e{capacidad de generalización} del cerebro humano.
%
%
\subsection{Modelo computacional de una neurona}
%
El modelo computacional básico de una neurona consiste en una
\e{función de activación} binaria cuyo valor de salida depende del
valor de una suma ponderada de las entradas: si el valor de la suma
alcanza un valor mínimo (umbral) $U$, entonces el valor de salida se
``activa'' al valor $1$.  Matemáticamente, este modelo de la neurona
se escribe
%
\begin{align}\label{e2:neuron-basic}
  y \tab = H\left( \sum_{j=1}{N} w_j x_j - U\right),
\end{align}
%
en donde $H$ es la función escalón unitario \cite{mcculloch-pitts}.
Imaginando que esta neurona es parte de una red (un nodo en un grafo),
en donde cada peso $w_j$ multiplica la señal proveniente de otra
neurona a través de una arista del grafo (conexión), resulta posible
efectuar una analogía entre este modelo artificial y una neurona
biológica: el peso $w_j$ representa la sinapsis, las conexiones entre
neuronas representan las hebras y dendritas, y la función de
activación representa la actividad neuronal que acontece en el soma.

Una forma general del modelo de una neurona $i$
(\iflatexml{}Figura~\ref{f2:neurona}\else\autoref{f2:neurona}\fi)
es
%
\begin{align}\label{e2:neuron-general}
  s_i \tab = f(v_i), \tabs v_i \tab = \sum_{j=0}^N w_j x_j,
\end{align}
%
en donde $v_i$ se denomina el \e{campo local inducido} de la neurona
$i$, y $f$ es una función de activación no lineal que acota el rango
de la salida $s_i$ a un intervalo conocido. Algunas funciones de
activación típicas son el escalón de Heaviside, la función
arcotangente, o alguna variante de la función sigmoidea
tal como
%
\begin{align}\label{e2:sigmoid-symmetric}
  f(v_i) = \frac{2}{1+e^{-v_j}}-1,
\end{align}
%
la cual posee un rango de salida simétrico en $(-1,1)$.  Esta función
monótona creciente y continuamente derivable presenta un equilibrio
entre un comportamiento lineal cerca del origen y no lineal para
magnitudes mayores de $v_i$.  El umbral $b_i$ de la neurona,
equivalente al valor $U$ en la
\iflatexml{}Ecuación~\ref{e2:neuron-basic}\else\autoref{e2:neuron-basic}\fi
se representa en este modelo como la ponderación de una entrada
constante $s_0=1$ a la neurona, de modo que
%
\begin{align}
  b_i\tab=-w_{i0}.
\end{align}
%
