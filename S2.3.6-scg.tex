%
\subsection{Variantes del algoritmo de retropropagación}
%
Si bien el algoritmo de retropropagación resulta fundamental en el
problema de calcular el gradiente de error $\nabla{\C{E}(n)}$ respecto
de los pesos $w_{ij}$, muchas veces la estrategia de ajuste de los
pesos de descenso por gradiente no resulta satisfactoria.  Como
ejemplo, la elección del parámetro velocidad de aprendizaje depende de
la forma de la función de error $\C{E}$, la cual a su vez depende de
los datos de entrenamiento.  Con frecuencia, se encuentra que la
velocidad de aprendizaje elegida es demasiado baja, añadiendo tiempo
innecesario al entrenamiento, o demasiado alta, generando oscilaciones
alrededor del mínimo local que impiden la convergencia.  Por ello, se
han desarrollado algoritmos alternativos que intentan salvar las
limitaciones del descenso por gradiente en el algoritmo de
retropropagación clásico.
%
\subsubsection{Retropropagación con término de momento}
%
%% Si la ``superficie'' de la función de error es ``chata'', una
%% velocidad de aprendizaje pequeña resultará en un tiempo de
%% convergencia muy elevado.  Similarmente, una velocidad de aprendizaje
%% grande posiblemente genere oscilaciones, previniendo que el error
%% caiga por debajo de un cierto valor.
%% Si bien la convergencia del algoritmo a un mínimo local puede probarse
%% en ocasiones, no existen garantías de encontrar el mínimo global de la
%% función de error.
Una estrategia simple que permite incrementar la velocidad de aprendizaje
evitando la inestabilidad es modificar la ``regla delta''
(\iflatexml{}Ecuación~\ref{e2:delta-rule}\else\autoref{e2:delta-rule}\fi)
añadiedo un \e{término de momento}
%
\begin{align}\label{e2:momentum-term}
  \Delta w_{ij}(n)\tab=-\eta\dpar{\C{E}(n)}{w_{ij}}{}
  +\mu\Delta w_{ij}(n-1).
\end{align}
%
El parámetro de momento $\mu$ controla el grado de influencia del paso
anterior en el paso actual: si el paso actual tiene el mismo sentido
que el anterior, el efecto del término de momento es aumentar la
velocidad de aprendizaje; si en cambio el paso actual es de sentido
opuesto al anterior, se reduce la velocidad de aprendizaje.

%% Esta técnica funciona bien en muchos problemas, aunque no significa un
%% incremento general de la estabilidad o de la velocidad de
%% convergencia. A veces, el descenso de gradiente sin término de momento
%% obtiene resultados iguales o mejores que con el término de momento.
%% Al utilizar aprendizaje con término de momento, probablemente se deba
%% disminuir la velocidad de aprendizaje para evitar inestabilidad en el
%% proceso de aprendizaje.
%
\subsubsection{Descenso más pronunciado}
%
En la ``regla delta'' básica
(\iflatexml{}Ecuación~\ref{e2:delta-rule}\else\autoref{e2:delta-rule}\fi),
la dirección de búsqueda del mínimo
viene dada por el negativo del gradiente
$-\nabla\C{E}(n)=-\dpar{\C{E}(n)}{w_{ij}}{}$, mientras que la
velocidad de aprendizaje $\eta$ determina la magnitud del paso a
efectuar.
La regla llamada del \e{descenso más pronunciado} consiste
en buscar, en cada iteración, una velocidad de aprendizaje óptima
$\eta(n)$ mediante una estrategia iterativa denominada \e{búsqueda en
  la línea}.  Básicamente, la búsqueda en la línea consiste en evaluar
la función de error resultante para distintos $\eta$ hasta encontrar
el óptimo.  La ``regla delta'' del descenso más pronunciado resulta
entonces
%
\begin{align}\label{e2:delta-rule-steepest}
  \Delta w_{ij}(n)\tab=-\eta(n)\nabla{\C{E}(n)}.
\end{align}
%
La búsqueda de $\eta(n)$ mediante búsqueda en la línea añade
complejidad computacional a cada iteración del algoritmo, aunque en
general redunda en una reducción del número de iteraciones necesarias
hasta alcanzar la convergencia.  Desde el punto de vista del usuario,
evita la necesidad de determinar el parámetro de entrenamiento $\eta$
mediante prueba y error.
%
\subsubsection{Gradiente conjugado}
%
Al utilizar la regla del descenso más pronunciado, se puede demostrar
que dos pasos de actualización sucesivos ocurren en direcciones
necesariamente perpendiculares.
%% Esto ocurre porque, en dos iteraciones sucesivas $(n)$ y $(n+1)$ los
%% gradientes $-\dpar{\C{E}(n)}{w_{ij}}{}$ $-\dpar{\C{E}(n)}{w_{ij}}{}$
Esta perpendicularidad entre iteraciones genera un ``zigzagueo'' en el
camino del descenso del gradiente. El algoritmo del \e{gradiente
  conjugado} (no escalado) aprovecha esta perpendicularidad
estableciendo como dirección de búsqueda una combinación de la
dirección de búsqueda anterior y del gradiente actual, ``atravesando''
el zigzag y logrando entonces pasos más pronunciados y directos.
La regla de actualización de los pesos mediante gradiente conjugado
se define por
%
\begin{align}\label{e2:delta-rule-conjugate}
  \Delta w_{ij}(n)\tab=\eta(n)\dd(n),
  \tabs \dd(n)\tab=-\nabla{\C{E}(n)}+\beta\dd(n-1),
\end{align}
%
donde $\eta(n)$ se determina mediante búsqueda en la línea y $\beta$
viene dado por la fórmula de Polak-Ribière:
%
\begin{align}\label{e2:polak-ribiere}
  \beta=
  \frac{(\nabla{\C{E}(n)}-\nabla{\C{E}(n-1)})\nabla{\C{E}(n)}}{(\nabla{\C{E}(n-1)})^2}.
\end{align}
%
%% La utilización del algoritmo del gradiente conjugado trae consigo un
%% mayor coste computacional de cada iteración y una convergencia en
%% unas pocas iteraciones en comparación con la retropropagación
%% clásica.
El mayor costo computacional de las iteraciones en el algoritmo del
gradiente conjugado se compensa con una convergencia mucho más rápida
en problemas de clasificación binarios en comparación con la
retropropagación clásica.
%% Sin embargo, tal como se detalla en
%% \cite{schiffmann}, esta técnica de optimización global puede
%% experimentar severos problemas de convergencia cuando se aplica a
%% tareas de aprendizaje más grandes.
%
\subsubsection{Gradiente conjugado escalado}
%
El algoritmo del gradiente conjugado escalado (\e{scaled conjugate
  gradient, SCG}) \cite{scg} es una mejora del algoritmo del gradiente
conjugado estándar. Mientras que el algoritmo del gradiente conjugado
estándar efectúa una búsqueda en la línea para determinar el tamaño de
paso óptimo $\eta(n)$, el algoritmo del gradiente conjugado escalado
utiliza una estrategia más eficiente de tipo Levenberg-Marquardt para
regular la velocidad de aprendizaje en cada iteración.
%% En pos de lograr brevedad en la exposición, se omiten los detalles de
%% la derivación de este algoritmo dada la gran cantidad de conceptos
%% teóricos necesarios para explicarlo con claridad.  El lector
%% interesado encontrará en el trabajo original de su creador \cite{scg}
%% una descripción detallada del método.

En la actualidad, junto al algoritmo Rprop descripto a continuación,
el algoritmo SCG es la forma de entrenamiento estándar del perceptrón
multicapa debido a su estabilidad y velocidad de entrenamiento.

%% Desde el punto de vista de la optimización numérica, el descenso por
%% gradiente efectuado por el algoritmo de retropropagación puede
%% mejorarse en varios aspectos.  El algoritmo del gradiente conjugado
%% escalado (SCG), propuesto por \cite{scg} en 1990, incorpora una serie
%% de mejoras que derivan en un entrenamiento notablemente más rápido.

%% Para llegar a una descripción del algoritmo SCG, resulta necesario
%% revisar algunos conceptos en los que se basa. Las descripciones a
%% continuación omiten, en pos de brevedad, detalles en las derivaciones
%% y fórmulas específicas. Para referencia, se refiere al lector a
%% \cite{haykin} (sección 4.16), \cite{riedmiller} y \cite{scg}.
