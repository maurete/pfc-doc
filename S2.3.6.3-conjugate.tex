%
\subsubsection{Gradiente conjugado}
%
Al utilizar la regla del descenso más pronunciado, se puede demostrar
que dos pasos de actualización sucesivos ocurren en direcciones
necesariamente perpendiculares.
%% Esto ocurre porque, en dos iteraciones sucesivas $(n)$ y $(n+1)$ los
%% gradientes $-\dpar{\C{E}(n)}{w_{ij}}{}$ $-\dpar{\C{E}(n)}{w_{ij}}{}$
Esta perpendicularidad entre iteraciones genera un ``zigzagueo'' en el
camino del descenso del gradiente. El algoritmo del \e{gradiente
  conjugado} (no escalado) aprovecha esta perpendicularidad
estableciendo como dirección de búsqueda una combinación de la
dirección de búsqueda anterior y del gradiente actual, ``atravesando''
el zigzag y logrando entonces pasos más pronunciados y directos.
La regla de actualización de los pesos mediante gradiente conjugado
se define por
%
\begin{align}\label{e2:delta-rule-conjugate}
  \Delta w_{ij}(n)\tab=\eta(n)\B{d}(n),
  \tabs \B{d}(n)\tab=-\nabla{\C{E}(n)}+\beta\B{d}(n-1),
\end{align}
%
donde $\eta(n)$ se determina mediante búsqueda en la línea y $\beta$
viene dado por la fórmula de Polak-Ribière:
%
\begin{align}\label{e2:polak-ribiere}
  \beta=
  \frac{(\nabla{\C{E}(n)}-\nabla{\C{E}(n-1)})\nabla{\C{E}(n)}}{(\nabla{\C{E}(n-1)})^2}.
\end{align}
%
%% La utilización del algoritmo del gradiente conjugado trae consigo un
%% mayor coste computacional de cada iteración y una convergencia en
%% unas pocas iteraciones en comparación con la retropropagación
%% clásica.
El mayor costo computacional de las iteraciones en el algoritmo del
gradiente conjugado se compensa con una convergencia mucho más rápida
en problemas de clasificación binarios en comparación con la
retropropagación clásica.
%% Sin embargo, tal como se detalla en
%% \cite{schiffmann}, esta técnica de optimización global puede
%% experimentar severos problemas de convergencia cuando se aplica a
%% tareas de aprendizaje más grandes.
