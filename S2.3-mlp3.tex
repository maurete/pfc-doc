%
%
\subsection{Aprendizaje}
%
%% En el contexto de las redes neuronales artificiales, el aprendizaje
%% puede ser visto como el problema de efectuar actualizaciones de los
%% pesos en las conexiones de la red de modo que ésta efectúe una tarea
%% deseada.

%% La red debe ``aprender'' los pesos en sus conexiones a partir de los
%% patrones de entrenamiento. El rendimiento en la clasificación mejora
%% progresivamente a medida que se van actualizando los pesos.

%% La habilidad de las redes neuronales de aprender de manera autoomática
%% a partir de ejemplos es quizá la caracteríßtica que hace a su
%% éxito. En lugar de seguir un conjunto de reglas especificadas por
%% expertos humanos, las redes neuronales artificiales ...

%% El algoritmo de aprendizaje utilizado por una red neuronal artificial
%% viene determinado por la arquitectura de la red.

%% El algoritmo de entrenamiento supervisado

%% %
%% %
%% \subsection{Entrenamiento del perceptrón multicapa}
%% %

El objetivo del entrenamiento en el perceptrón multicapa es ajustar
los pesos de la red de forma que ésta efectúe una transformación
de las entradas a las salidas deseadas. Esta transformación viene dada
por el conjunto de entrenamiento $D$: cada ejemplo $(\xx_j,\yy_j)$ en
$D$ consta de un vector de activación $\xx_j$ y una salida deseada
$\yy_j$.
%% Una vez entrenada la red, cuando
%% ésta recibe una activación $\xx_i$, el vector de salida
%% debería ser $\yy_i$.
La distancia entre la salida deseada y la salida efectiva de la red se
mide con la función de costo (o de energía) $E$:
%
\begin{align}
  E = \frac{1}{2}\sum_{j=1}^{\ell}\|\yy_j-\B{s}_j\|,
\end{align}
%
en donde el vector $\B{s}_j$ es el vector compuesto por las salidas de
cada neurona en la capa de salida. La función $E$ puede
interpretarse como una medida de \e{aptitud} de los pesos de la red al
conjunto $D$, de modo que satisfacer el objetivo del aprendizaje es
equivalente a encontrar un mínimo global de $E$.

En cada iteración $t=0,1,2,\ldots$, los pesos de la red
se modifican a lo largo de una dirección de búsqueda
$d(t)$ %, llevando los pesos en la dirección del mínimo estimado:
%
\begin{align}
  \Delta w(t)\tab=\epsilon * d(t), \tabs w(t+1) \tab= w(t)+\Delta w(t).
\end{align}
%
El parámetro $\epsilon$ es un número pequeño que escala la magnitud de
la modificación de los pesos y se denomina \e{velocidad de
aprendizaje}.  Para encontrar la dirección de búsqueda $d(t)$ se
utiliza el gradiente $\nabla{}E=\dpar{E}{w}{}$, calculado mediante el
algoritmo de retropropagación.

Considerando una neurona $i$ en la capa de salida, sea $s_i(t)$ la
salida de la misma en respuesta al estímulo $\xx(t)$ aplicado en la
capa de entrada. La \e{señal de error} producida en la salida de esta
neurona se define según
%
\begin{align}\label{4.2}
  e_i(t)=s_{i}(t)-y_{i}
\end{align}
%
en donde $y_{i}$ es el $i$-ésimo elemento del vector de respuesta
deseada $\B{y}$. La \e{energía de error instantáneo} de la neurona
$i$ se define según
%
\begin{align}
\label{4.3}
  \C{E}_i(t)=\frac{1}{2}e^2_i(t).
\end{align}
%
Sumando las contribuciones error-energía de todas las neuronas en la
capa de salida, se expresa la \e{energía total de error instantáneo}
de la red entera como
%
\begin{align}
\label{4.4}
  \C{E}(n)&=\sum_{j\in C}\C{E}_j(n) \\
  &=\frac{1}{2}\sum_{j\in C}e^2_j(n).
\end{align}
%
en donde el conjunto $C$ incluye todas las neuronas en la capa de
salida. La energía de error promedio sobre el conjunto de entrenamiento,
denominada \e{riesgo empírico}, se define según
%
\begin{align}
\label{4.5}
  \C{E}_{\T{av}}(N)&=\frac{1}{N}\sum_{n=1}^N\C{E}(n) \\
  &=\frac{1}{2N} \sum_{n=1}^N \sum_{j\in C}e^2_j(n).
\end{align}
%
Naturalmente, tanto la energía de error instantáneo como la energía de
error promedio son funciones de todos los pesos sinápticos del
perceptrón multicapa. Esta dependencia funcional no fue incluida de
forma explícita en la definición de $\C{E}(n)$ y $\C{E}_{\T{av}}(N)$
en pos de simplificar la notación.
%
%
\subsection{El algoritmo de retropropagación}
%
El algoritmo de retropropagación es un algoritmo específico para el
perceptrón multicapa que efectúa el cálculo del gradiente $\nabla E$
necesario para el ajuste de los pesos de la red en el entrenamiento.
Aprovechando la arquitectura del MLP, el algoritmo de retropropagación
calcula el gradiente de la función de error comenzando por la capa de
salida y \e{propagando} el error $E$ en dirección inversa hasta la
primer capa oculta.

La idea básica para el cálculo de las derivadas parciales
$\dpar{E}{w_{ij}}{}$ para cada peso en la red consiste en aplicar
repetidamente la regla de la cadena
%
\begin{align}\label{e2:deriv-E-wrt-w}
  \dpar{E}{w_{ij}}{} = \dpar{E}{s_{i}}{}\dpar{s_i}{w_{ij}}{},
\end{align}
%
donde
%
\begin{align}\label{e2:deriv-s-wrt-w}
  \dpar{s_i}{w_{ij}}{} = \dpar{s_i}{v_{i}}{}\dpar{v_i}{w_{ij}}{} =
  f'(v_i) s_j .
\end{align}
%
El cálculo de $\dpar{E}{s_i}{}$, que representa la influencia
de la salida $s_i$ de la unidad $i$ en el error global $E$,
se distinguen los casos siguientes:
%
\begin{itemize}
\item Si la neurona $i$ está en la capa de salida:
  %
  \begin{align}\label{e2:deriv-E-wrt-s-out} %mlp4
    \dpar{E}{s_i}{}=\frac{1}{2}\dpar{(t_i-s_i)^2}{s_i}{}=-(t_i-s_i)
  \end{align}
  %
\item Si la neurona $i$ se ubica en una capa oculta, el cálculo de
  $\dpar{E}{s_i}{}$ resulta más complejo. Aplicando nuevamente la
  regla de la cadena, se tiene:
  %
  \begin{align}\label{e2:deriv-E-wrt-s-hid} %mlp5
    \dpar{E}{s_i}{} \tab= \sum_{k\in\C{S}(i)}
      \dpar{E}{s_k}{}\dpar{s_k}{s_i}{} \notag\\
    \tab= \sum_{k\in\C{S}(i)}
      \dpar{E}{s_k}{}\dpar{s_k}{v_k}{}\dpar{v_k}{s_i}{} \notag\\
    \tab= \sum_{k\in\C{S}(i)} \dpar{E}{s_k}{} s'(v_k) w_{ki},
  \end{align}
  %
\end{itemize}
donde $\C{S}(i)$ denota el conjunto de todas las unidades $k$ en las
sucesivas capas para la cual la unidad $i$ tiene una conexión
pesada distinta de cero $w_{ki}$.

En la
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-s-hid}\else\autoref{e2:deriv-E-wrt-s-hid}\fi
se asume que los valores $\dpar{E}{s_k}{}$ para todas las capas
sucesivas a la neurona $k$ es conocido. Ésta es la razón por la cual
el agoritmo comienza el cálculo de $\dpar{E}{s_k}{}$ en la capa de
salida
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-s-hid}\else\autoref{e2:deriv-E-wrt-s-hid}\fi,
continuando con las capas ocultas en orden inverso.
%% Este requerimiento
%% se satisface simplemente comenzando el cálculo de $\dpar{E}{s_k}{}$ a
%% partir de la capa de salida (\refer{mlp4}) y luego continuar el cálculo
%% para las unidades en la capas pecedentes según la \refer{mlp5}.
Por ello se dice que la información del gradiente de error se propaga hacia atrás
a partir de la capa de salida hasta la primer capa
oculta. De allí el nombre `` retropropagación''.

El desarrollo del algoritmo de retropropagación a mediados de los años
80 representó un hito en el campo de las redes neuronales, ya
que brindó un método computacionalemnte eficiente, usable, para
entrenar el perceptrón multicapa.
%
\subsubsection{Descenso del gradiente}
%
Una vez que se conocen las derivadas parciales, el paso siguiente en
el aprendizaje por retropropagación es calcular el ajuste resultante a
los pesos. En su forma más simple, la actualización de los pesos es un
paso escalado en la dirección opuesta al gradiente.  En otras
palabras, el negativo del gradiente se multiplica por una constante
pequeña $\epsilon$, la velocidad de aprendizaje. Esta técnica de
minimización se conoce comúnmente como el ``descenso del gradiente'':
%
\begin{align}\label{mlp6}
  \Delta w(t) = - \epsilon \nabla E(t),
\end{align}
%
o, para cada peso,
%
\begin{align}\label{mlp6}
  \Delta w_{ij}(t) = - \epsilon \dpar{E}{w_{ij}}{}(t).
\end{align}
%
Si bien la regla básica de aprendizaje es simple, con frecuencia
resulta difícil elegir en forma adecuada la velocidad de
aprendizaje. Una buena elección depende de la forma de la función de
error, que obviamente cambia con cada problema de aprendizaje
planteado, además de ser una función de dimensionalidad elevada.  Una
velocidad de aprendizaje pequeña resultará en un tiempo elevado de
convergencia en una función de error ``chata'', mientras que una velocidad
de aprendizaje grande posiblemente genere oscilaciones, previniendo
que el error caiga por debajo de un cierto valor.  Más aún, si bien la
convergencia a un mínimo local puede probarse en determinadas
circunstancias, no existen garantías que el algoritmo encuentre un
mínimo global de la función de error.

Otro problema con el descenso por gradiente es la influencia
``contraintuitiva'' de la derivada parcial sobre el tamaño del paso en
el ajuste de los pesos: si la función de error es ``playa'', la
derivada es relativamente pequeña, resultando en un paso pequeño.  Por
otro lado, ante la presencia de pendientes bruscas en la función de
la energía, en donde se deberían tomar pesos pequeños dada la
elevada pendiente, las derivadas ``grandes'' llevan a pasos grandes,
llevando posiblemente la búsqueda a una región completamente diferente
del espacio de los pesos.

Una idea para hacer el entrenamiento más estable es la adición de un
\e{término de momento}
%
\begin{align}\label{mlp8}
  \Delta w_{ij}(t) = - \epsilon \dpar{E}{w_{ij}}{}(t)
    + \mu \Delta w_{ij}(t-1).
\end{align}
%
El parámetro de momento $\mu$ escala la influencia del paso anterior
de actualización de los pesos en el paso actual. Esta técnica funciona
bien en muchas tareas de aprendizaje, aunque no significa un
incremento general de la estabilidad o de la velocidad de
convergencia.  A veces, el descenso de gradiente sin término de
momento obtiene iguales o mejores resultados que con el término de
momento. Usualmente, al utilizar aprendizaje con término de momento se
deberá disminuir la velocidad de aprendizaje para evitar inestabilidad
en el proceso de aprendizaje.
%
\subsubsection{Aprendizaje en línea y aprendizaje por época}
%
Según en qué momento se efectúa la actualización de los pesos de la
red neuronal, se distinguen dos casos: el aprendizaje en línea y el
aprendizaje por época.

En el aprendizaje en línea, o aprendizaje por patrón, se
propaga el error y se efectúa una actualización de los pesos de la red
luego de cada
presentación de un elemento del conjunto de aprendizaje.
Este método se denomina también aprendizaje
estocástico, y minimiza el error total
minimizando el error para cada par de patrones, siendo que estas cosas
no son de hecho lo mismo. Este método funciona especialmente bien para
grandes conjuntos de entrenamiento que contengan información
redundante.

En el aprendizaje por época, la actualización de los pesos de la red
se efectúa al final de cada época, 
sumando los gradientes de todo el conjunto de entrenamiento.  Cada
actualización de los pesos intenta minimizar la sumatoria del error
del conjunto de patrones, en otras palabras, la función de error en
\refer{mlp1}. Los procedimientos adaptativos descritos en adelante
utilizan este tipo de aprendizaje, ya que la información de gradiente
sumada contiene información más fiable acerca de la forma de la
función de error como un todo \cite{riedmiller}.  Algunos autores, sin
embargo, sugieren que la mejor técnica es la del aprendizaje en línea
\cite{haykin}.
En el presente trabajo se considera en todos los casos el aprendizaje
por épocas.
%
\subsubsection{Técnicas adaptativas}
%
A la fecha, se han propuesto muchas técnicas para afrontar los
problemas inherentes del método de descenso por gradiente. La mayoría
tienen sus bases de la disciplina de la teoría de
optimización.

Estas técnicas se pueden dividir en dos
categorías. Aquellos algoritmos que utilizan un conocimiento global
del estado de la red como un todo, tal como la dirección del vector de
actualización de los pesos ``entero'', son llamadas técnicas globales.
Existen muchos ejemplos donde los algoritmos de aprendizaje utilizan el
conocimiento global \cite{salomon,moeller}.

Por contraste, las estrategias de adaptación locales se basan
únicamente en información específica a los pesos, tal como el
comportamiento temporal de la derivada parcial de este peso. El
enfoque local se relaciona más naturalmente con el concepro de
procesamiento distribuido en el que los cálculos pueden efectuarse en
paralelo.  Más aún, en muchos casos se encuentra que las estrategias
locales funcionan mucho mejor que las globales, a pesar del hecho que
utilizan menos información y de que normalmente son mucho más rápidas
y fáciles de calcular \cite{schiffmann}.
%
\subsubsection{Método del gradiente conjugado}
%
Desde el punto de vista de la optimización numérica, el entrenamiento
por descenso de gradiente puede ``mejorarse'' de modo que efectúe el
tamaño de paso óptimo en cada iteración encontrando una ``velocidad de
aprendizaje'' $\epsilon$ óptima en cada iteración mediante la técnica
de ``búsqueda en la línea''. Este es un problema de optimización en
una dimensión que requiere la propagación del error en cada cálculo.
Al aplicar la búsqueda en la línea, se puede demostrar que dos pasos
de actualización sucesivos ocurren en direcciones necesariamente
perpendiculares. Asumiendo que se conoce un tal $\epsilon$ óptimo,
esto implica que
%
\begin{align*}
  \dpar{E(w(t+1))}{\epsilon}{}=0,
\end{align*}
%
esto es, se ha encontrado un mínimo en la dirección
de búsqueda de la búsqueda en la línea. Entonces, se tiene que
%
\begin{align}
\label{mlp9}
  \dpar{E(w(t+1))}{\epsilon}{} \tab= \dpar{E(w(t+1))}{w(t+1)}{}
    \dpar{w(t)+\epsilon\cdot d(t)}{\epsilon}{} \notag\\
  \tab= \nabla E(t+1) d(t) \notag\\
  \tab=0.
\end{align}
%
Esto significa que el nuevo gradiente $\nabla{}E(t+1)$, que determina
la nueva dirección $d(t+1)$, y la dirección actual $d(t)$ son
perpendiculares. Esta relación de ortogonalidad es la que aprovecha
el método del gradiente conjugado.
%
\begin{quote}
  Encontrar una velocidad de aprendizaje óptima es un procedimiento
  iterativo costoso, luego no queremos perder este esfuerzo en los
  pasos siguientes.
\end{quote}
%
Resulta posible que la condición \refer{mlp9} también resulta válida
para la actualización de pesos subsiguiente
%
\begin{align}
\label{mlp10}
  d(t)\nabla{}E(t+2)=0.
\end{align}
%
Se puede demostrar que la condición \refer{mlp10} se satisface si existe
una matriz hessiana $H$ con las derivadas segundas de los pesos tal
que
%
\begin{align}\label{mlp11}
  d(t) H d(t+1) =0.
\end{align}
%
Dos vectores que satisfacen la condición \refer{mlp11} se denominan
``conjugados''.

Para determinar una nueva dirección de búsqueda que cumpla con la
condición \refer{mlp11} se establece
%
\begin{align}
  d(t+1) = -\nabla{}E(t+1)+\beta d(t)
\end{align}
%
Esto significa que la nueva dirección de búsqueda es una combinación
de la dirección indicada por el gradiene y por la nueva direccin de
búsqueda. El parámetro $\beta$ se calcula, por ejemplo, de acuerdo a
la regla de Polak-Ribiere. Mediante una búsqueda en la línea, se
deberá encontrar una velocidad de aprendizaje óptima.

Según varios reportes, el mayor costo computacional de la regla del
gradiente conjugado (búsqueda en la línea + regla de Polak-Ribiere) se
compensa con una convergencia mucho más rápida en problemas de
clasificación binarios comparada con el método de
retropropagación. Sin embargo, tal como se detalla en
\cite{schiffmann}, esta técnica de optimización global puede
experimentar severos problemas de convergencia cuando se aplica a
tareas de aprendizaje más grandes.
%
\subsubsection{Rprop}
%
El método Rprop (\eng{Resilient backpropagation}, ``retropropagación
resiliente'') \cite{rprop} es un esquema local cuyo principio básico
es eliminar la influencia del tamaño del gradiente sobre el
tamaño de incremento del peso. Consecuentemente, sólo el signo de la
derivada es considerado para indicar la dirección de la actualización
de los pesos.

Al principio, todos los valores de actualización se establecen a
$\Delta_0$, que es uno de los dos parametros de Rprop. Ya que
$\Delta_0$ determina directamente el tamaño de la primera
actualización de los pesos, se debe elegir su valor de acuerdo al
tamaño de los mismos pesos. La elección de este valor es más bien no
crítica, ya que se va adaptando a medida que el aprendizaje procede.

Con el objeto de prevenir que los pesos se vuelvan demasiado grandes,
se define el segundo parámetro $\Delta_{T{máx}}$, que limita el tamaño
máximo de la actualización de los pesos. Comúnmente, la convergencia
es insensible también al valor de este parámetro. Sin embargo, para
algunos problemas puede resultar ventajoso establecer este valor más bajo
con el objetivo de permitir sólo actualizaciones de pesos cautas.

El tamaño del cambio de los pesos se determina por un
valor de actualización específico para cada peso $\Delta_{ij}$:
%
\begin{align}
\label{mlp20}
  \Delta{}w_{ij}(t) = 
  \begin{cases}
    -\Delta_{ij}(t), & \T{si } \dpar{E}{w_{ij}}{}(t) > 0 \\
    +\Delta_{ij}(t), & \T{si } \dpar{E}{w_{ij}}{}(t) < 0 \\
    0, & \T{en otro caso}
  \end{cases}
\end{align}
%
El segundo paso del método Rprop es determinar los valores nuevos
$\Delta_{ij}(t)$. Esto se basa en un proceso de adaptación dependiente
del signo
%
\begin{align}
\label{mlp20}
  \Delta{}_{ij}^{(t)} = 
  \begin{cases}
    \eta^+\,\Delta_{ij}^{(t-1)}, & \T{si }\dpar{E}{w_{ij}}{}^{(t-1)}
      \dpar{E}{w_{ij}}{}^{(t)} > 0 \\
    \eta^-\,\Delta_{ij}^{(t-1)}, & \T{si }\dpar{E}{w_{ij}}{}^{(t-1)}
      \dpar{E}{w_{ij}}{}^{(t)} < 0 \\
    \Delta_{ij}^{(t-1)}, & \T{en otro caso},
  \end{cases}
  && 0<\eta^-<1<\eta^+.
\end{align}
%
Los factores de incremento y decremento se establecen en $\eta^+=1,2$
y $\eta^-=0.5$ respectivamente. Estos valores se basan en
consideraciones teóricas y evaluaciones empíricas \cite{riedmiller},
reduciendo el número de parámetros libres a 2: $\Delta_0$ y
$\Delta_{T{máx}}$.

Para resumir, el principio básico de Rprop es la adaptación directa de
los valores de actualización de los pesos $\Delta_{ij}$. Contrastando
con los algoritmos que ajustan la velocidad de aprendizaje, Rprop
modifica directamente el tamaño del paso introduciendo el concepto de
valores de actualización ``resilientes''. Como resultado, el esfuerzo
de adaptación no se ``desenfoca'' como resultado de un comportamiento
impredecible del gradiente. Dada la claridad y simplicidad de las
leyes de aprendizaje, se introduce un pequeño incremento en el tiempo de
entrenamiento, en comparación con la retropropagación ordinaria.

Rprop sufre de los mismos problemas que el resto de los algoritmos de
aprendizaje adaptativos. Ya que la adaptación se basa en una
estimación de la topología de la función de error, tanto la adaptación
como la actualización de los pesos se puede efectuar recién luego de
que toda la información de gradiente esté disponible, en otras
palabras, luego de que cada patrón haya sido presentado y el gradiente
de la suma de los errores de todos los patrones es conocida.

Consecuentemente, los procedimientos de aprendizaje adaptativos
típicamente se basan en el ``aprendizaje batch''. Esto posiblemente
reduce su eficacia en conjuntos de entrenamiento redundantes
comparados a un descenso del gradiente estocástico, y presenta
problemas con el uso de conjuntos de entrenamiento variables.

Más aún, a un esquema de adaptación restringido local inherentemente le
falta la visión del todo que las técnicas globales podrían llegar a
tener. Si por ejemplo la dirección óötima del mínimo se sitúa sobre
una diagonal, un esquema local tratará de reducir el error en cada
dimensión con pasos pequeños en lugar de incrementar el paso compuesto
sobre la diagonal, que sería el enfoque más apropiado en este caso.

Sin embargo, los resultados reportados en las pruebas
\cite{riedmiller} muestran las propiedades favorables de las
estrategias de adaptación locales en las aplicaciones prácticas.
