
\subsection{Búsqueda exhaustiva}
La estrategia de búsqueda exhaustiva define como función objetivo el
valor $G_m$ promedio de validación cruzada, y selecciona
hiperparámetros óptimos probando distintos valores posibles de cada
hiperparámetro en un rango preestablecido.  Dado que la naturaleza de
los hiperparámetros es diferente según el clasificador, el algoritmo
de búsqueda también difiere en cada caso.

\subsubsection{Perceptrón multicapa}
El hiperparámetro a optimizar en el caso MLP es el número de neuronas
en la capa oculta, una variable discreta no negativa.  La búsqueda
consiste en entrenar 20 clasificadores con $h$ capas ocultas, variando
$h$ entre los valores

\begin{align}
  \label{mlp-hidden-tries}
  h=(0,1,2,3,4,5,7,9,11,14,19,24,32,41,54,70,91,118,154,200).
\end{align}
Estos valores representan una escala aproximadamente logarítmica entre
0 y 200. Una vez probados todos los clasificadores con los conjuntos
de validación cruzada, se selecciona el $h$ para el cual se obtuvo el
mayor valor de $G_m$ en promedio.

\subsubsection{Máquina de vectores de soporte}
En el caso SVM, los hiperparámetros a optimizar son las variables
reales no negativas $C$ y $\gamma$--sólo en caso de utilizar un núcleo
RBF. En la práctica, la búsqueda se efectúa en el espacio logarítmico
$\log{}C$, $\log\gamma$, eliminando el requerimiento de no negatividad
de las variables.

El algoritmo de búsqueda se basa en la técnica de \e{búsqueda en la
  grilla} propuesta por \citeauthor{hsu} en \cite{hsu}.  La idea
básica es considerar la combinación de hiperparámetros $(\log
C,\log\gamma)$ como puntos en el plano.  Seleccionando una serie de
puntos espaciados regularmente en el plano $\log-C\gamma$ (la
``grilla''), se entrena y evalúa el clasificador sobre cada punto,
interpolando la grilla en las cercanías de los puntos donde se obtiene
el mayor rendimiento para continuar la búsqueda.

En primer lugar, se definen los puntos de muestreo para los
hiperparámetros

\begin{align*}
  \log_2 C &= -5, -3, -1, 1, \ldots, 15, & \log_2\gamma &= -15,-13,
  -11, \ldots,3,
\end{align*}
que definen la grilla inicial. Para cada punto $(C_i,\gamma_j)$, se
entrena y prueba un clasificador SVM, obteniendo un ${G_m}_{ij}$
promedio de validación cruzada.

En etapas sucesivas, se interpola la grilla alrededor de los puntos
donde se obtuvieron los mayores valores $G_m$. Se definen tres algoritmos
heurísticos que determinan cuáles puntos interpolar:

\begin{itemize}
  %
\item \e{Zoom}: En un primer paso, se convoluciona la grilla con una
  ventana cuadrada uniforme de valor unitario, obteniendo una versión
  ``suavizada'' de la grilla. Se interpolan puntos en una región
  cuadrada centrada en el punto con mayor valor $G_m$ ``suavizado'',
  con precisión (refinamiento) y amplitud especificados por el
  usuario, por defecto, se utiliza una precisión de $0.5$ la precisión
  actual, y una amplitud de $0.5$ la amplitud actual en cada
  dimensión.
\item \e{Umbral}: A partir de un umbral $G_m$ definido por el usuario,
  por defecto el percentil 90, se interpola en cada dimensión al $0.5$
  de la precisión actual alrededor de los puntos por encima del
  umbral.
\item \e{$n$-mejores}: Similar al umbral, se seleccionan $n$ puntos
  con los mayores valores de $G_m$. Se interpola la grilla en ambas
  dimensiones al $0.5$ de la precisión actual alrededor de estos
  puntos.
\end{itemize}
Una vez interpolada la grilla, se calcula el valor $G_m$ promedio de
validación cruzada para los nuevos puntos. El procedimiento de
refinamiento se repite hasta un máximo de $N$ iteraciones o hasta
satisfacer un criterio de corte.

La búsqueda en la grilla tiene la ventaja de ser conceptualmente
simple, sin embargo, se torna inviable cuando el vector de parámetros
$\B{\theta}$ contiene más de 2 elementos. Dado que se trata de un
método de búsqueda exhaustiva, resulta generalmente lento.  Cuando se
trabaja con un clasificador SVM con núcleo lineal, la grilla en
cuestión tiene una única dimensión: la del hiperparámetro $\log C$.
