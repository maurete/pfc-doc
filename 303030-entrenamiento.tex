%
\subsection{Entrenamiento}
%
Una vez determinado el número de neuronas en la capa oculta mediante
alguna de las estrategias descriptas, se inicializa aleatoriamente una
nueva red con la topología correspondiente.
Luego se procede al entrenamiento de esta red mediante un algoritmo de
retropropagación.
Por defecto se utiliza el algoritmo {Rprop} \cite{rprop}, aunque
también se permite especificar otros algoritmos, tal como el de
gradiente conjugado escalado (SCG) \cite{scg}.

La regularización del modelo se efectúa siguiendo una estrategia de
``corte prematuro'' (\eng{early stopping}), que consiste en separar un
$20\%$ de los ejemplos de entrenamiento seleccionados al azar, y
\pdfmargincomment[color=Red,open=true]{falta cita para esto}
deteniendo el entrenamiento cuando se detecta que el número de errores
de clasificación sobre estos ejemplos alcanza un valor mínimo.

En las pruebas efectuadas durante la codificación, se pudo observar
que en algunos casos la inicialización aleatoria de la red condiciona
negativamente al algoritmo de retropropagación, haciendo que éste
converja a un mínimo local y resultando en una solución subóptima.
Para contrarrestar este efecto, el método permite incorporar al modelo
múltiples redes con igual topología e inicializaciones aleatorias
diferentes.
La salida del modelo en este caso se determina calculando la moda
entre las salidas de las múltiples redes que lo componen.
