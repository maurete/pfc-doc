\subsubsection{Algoritmo de optimización}
Dado el conjunto de entrenamiento $D$, en cada punto de evaluación
$\Btheta$ el error empírico $E(\Btheta)$ se calcula del siguiente modo

\begin{enumerate}
\item Aplicando validación cruzada, entrenar un modelo SVM para cada
  conjunto de estimación.
\item Calcular las salidas $f_k$ (\autoref{fk}) de los
  modelos para todos los ejemplos $\xx_k$ en las respectivas
  particiones de validación.
\item Encontrar valores óptimos de los parámetros $A$ y $B$, partiendo
  de valores iniciales $A_0=1$, $B_0=0$ (\autoref{abproblem}).
\item Calcular las probabilidades estimadas $\hat{p}_k$ y con ellas,
  el error empírico de cada ejemplo $E_k$ y global $E$.
\item Calcular el gradiente $\nabla{}E$.
\end{enumerate}
Partiendo de un punto inicial $\Btheta^0$, la búsqueda procede en cada
punto $\Btheta^i$ evaluando la función objetivo y determinando un
nuevo punto $\Btheta^{i+1}$ a partir del valor $E^i=E(\Btheta^i)$ y la
información disponible en el gradiente $\nabla{}E^i$ hasta satisfacer
algún criterio de corte, por ejemplo

\begin{align*}
  |E^i-E^{i-1}|&<\eta, & \|\nabla E^i\| < \nu,
\end{align*}
donde $\eta$ y $\nu$ son números pequeños.  Este esquema es
implementado en la estrategia de minimización del error empírico
mediante el algoritmo de optimización BFGS \cite{bfgs}.
