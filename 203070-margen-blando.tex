%
%
\subsection{SVM de margen blando}
%
La máquina de vectores de soporte llamada ``de margen blando''
incorpora regularización al modelo, permitiendo la clasificación
errónea de algunos ejemplos de entrenamiento.
%% Al aplicar regularización, los modelos resultantes son más simples, y
%% además, permite entrenar el clasificador con datos de entrenamiento
%% que no son separables.
%
%% En la máquina de vectores de soporte sin regularización (``margen
%% duro''), el objetivo del entrenamiento es reducir la complejidad del
%% modelo resultante: esto puede verse a partir de la interpretación
%% geométrica de $\ww$: una gran norma de $w$ corresponde a un pequeño
%% margen geométrico $\rho=1/\|\ww\|$, y consecuentemente a un modelo más
%% complejo.
Esto se logra agregando al objetivo de minimización una medida del
error de clasificación de cada ejemplo $(\xx_i,y_i)$ a través de una
variable de holgura no negativa $\xi_i$, que es la función de pérdida
``bisagra''
%
\begin{align}
  \xi_n = \max\{0,1-y_n(\langle{}\ww,\Phi(\xx_i)\rangle+b)\}.
\end{align}
%
La variable $\xi_i$ mide la violación funcional del margen para cada
ejemplo $(\xx_i,y_i)$, indicada por la distancia al hiperplano de
separación.
Por definición, $\xi_i$ es mayor a cero sólo cuando el modelo
clasifica incorrectamente el ejemplo, esto es, cuando
$h(\xx_i)\neq{}y_i$.
Luego, se introduce un hiperparámetro de regularización $C>0$ que
ajusta el balance entre complejidad del modelo y la cantidad de error,
lo que deriva en el problema de optimización ``primal''
%
\begin{equation}\label{svm-primal-blando}
  \begin{aligned}
    \min_{\ww,b,\xi} \quad & \frac{1}{2}\|\ww\|^2+C\sum_{i=1}^{\ell}\xi_i\\
    \T{sujeto a} \quad &
    y_i\left(\langle{}\ww,\BPhi(\xx_i)\rangle+b\right) \geq 1-\xi_i, i=1,\ldots,\ell, \\
    & \xi_i \geq 0, i=1,\ldots,\ell.
  \end{aligned}
\end{equation}
%
%
%% La SVM de margen blando (con regularización ``L1'' o ``de norma 1'')
%% es el tipo de clasificador SVM más utilizado.  Consiste en un
%% clasificador de margen máximo con el truco del núcleo, tal como la SVM
%% de margen duro. A diferencia de ésta, sin embargo, la SVM de margen
%% blando contiene un término de regularización que controla la
%% ``cantidad de error'' permitido al modelo, obteniendo un balance entre
%% el error de entrenamiento y la complejidad del modelo obtenido.  La
%% introducción del término de regularización a la SVM de margen rígido
%% permite su aplicación a problemas no separables en el espacio imagen,
%% ya que se permiten errores de clasificación en el entrenamiento.
%
%% Como medida de complejidad del modelo
%% $h(\xx)=\T{signo}(\langle{}\ww,\BPhi(\xx)\rangle{}+b)$
%% se utiliza el valor $\frac{1}{2}\|\ww\|^2$, que es la misma función optimizada en
%% la SVM de margen rígido.
%
%% Otra forma de ver este concepto es que para valores pequeños de
%% $\|w\|$, la función $\langle{}\ww,\BPhi(\xx)\rangle+b$ es suave en el
%% espacio de entrada, y para valores mayores de $\|w\|$ se vuelve más
%% ``abrupta'' (en el sentido que posee derivadas más grandes).
%
%% Como medida del error de entrenamiento, se utiliza una función de
%% pérdida $L:X\times\RR\times\RR\rightarrow\RR^{\geq0}$.
%
Para obtener la forma dual de este problema, en primer lugar se
calcula el funcional lagrangiano
%
\begin{equation}
  \begin{aligned}
    \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta) &= \frac{1}{2} \|\ww\|^2
        + C \sum_{i=1}^{\ell} \xi_i \\
    &\quad\quad
        + \sum_{i=1}^{\ell} \alpha_i (1-\xi_i-y_i(\langle\ww,\BPhi(\xx_i)\rangle+b))
        - \sum_{i=1}^{\ell} \beta_i\xi_i
  \end{aligned}
\end{equation}
%
En el mínimo, se establecen las derivadas respecto de las variables
$\ww$, $b$, $\Bxi$ a cero y se obtiene
%
\begin{align}
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{\ww}{}
    &=\ww-\sum_{i=1}^\ell y_i\alpha_i\BPhi(\xx_i) = 0
    &\Rightarrow&& \ww &= \sum_{i=1}^\ell y_i\alpha_i\BPhi(\xx_i),
  \\
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{b}{}
    &=-\sum_{i=1}^\ell y_i\alpha_i = 0,
    &\Rightarrow&& \sum_{i=1}^\ell y_i\alpha_i &= 0,
  \\
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{\Bxi_i}{}
    &=C-\alpha_i-\beta_i = 0
    &\Rightarrow&& \alpha_i &\leq C.
\end{align}
%
Reescribiendo estos resultados en el problema dual, se tiene
%
\begin{equation}
\begin{aligned}\label{svmprob-dual-soft}
    \max_{\Balpha} \quad
    & \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha\\
    \T{sujeto a}\quad & \yy^T\Balpha = 0, \\
                      & 0\leq\alpha_i\leq C, \T{ para todo } i\in {1,\ldots,\ell }.
\end{aligned}\end{equation}
%
Este problema es idéntico a (\ref{e2:svm-hard-margin}), salvo por la
restricción adicional $\alpha_i\leq C$.
La \MVS{} que resuelve este problema en el entrenamiento es la más
utilizada, y se denomina también SVM ``de norma 1'' o ``con
regularización L1'', ya que la variable holgura $\xi_i$ se incorpora
al problema con potencia $1$.
Una variante llamada ``con regularización L2'' utiliza en su lugar la
norma $2$ del vector holgura $\B{\xi}=(\xi_1,\ldots,\xi_\ell)^T$,
correspondiente a una función de pérdida de bisagra al cuadrado.
En este caso, el objetivo del problema primal se define como
%
\begin{align}\label{svm-l2}
  \frac{1}{2}\|w\|^2+\frac{C}{2}\sum_{n=1}^{\ell}\xi_n^2,
\end{align}
%
sujeto a las mismas restricciones que el problema
(\ref{svm-primal-blando}).
