%
%
\subsection{El algoritmo de retropropagación}
%
El algoritmo de retropropagación es un algoritmo específico para el
perceptrón multicapa que efectúa entrenamiento ajustando los pesos
$w_{ij}$ de la red en el sentido que minimiza la función de error
$\C{E}$, a partir de la información obtenida del gradiente
$\nabla\C{E}=\dpar{\C{E}}{w_{ij}}{}$.

%% Aprovechando la arquitectura del MLP, el algoritmo de retropropagación
%% calcula el gradiente de la función de error comenzando por la capa de
%% salida y \e{propagando} el error $E$ en dirección inversa hasta la
%% primer capa oculta.

La idea básica del algoritmo de retropropagación es aplicar a cada
peso sináptico $w_{ij}(n)$ de la red una corrección
$\Delta{}w_{ij}(n)$ proporcional a la derivada parcial
$\dpar{\C{E}(n)}{w_{ij}}{}$.  Según la regla de la cadena, esta
derivada parcial puede escribirse
%
\begin{align}\label{e2:deriv-E-wrt-w-expanded}
  \dpar{\C{E}(n)}{w_{ij}}{} = \dpar{\C{E}(n)}{s_i(n)}{}
  \dpar{s_i(n)}{v_i(n)}{} \dpar{v_i(n)}{w_{ij}}{}.
\end{align}
%
Diferenciando ambos lados de la
\iflatexml{}Ecuación~\ref{e2:error-energy-neuron}
\else\autoref{e2:error-energy-neuron}\fi respecto de $s_i(n)$, se tiene
%
\begin{align}\label{e2:deriv-E-wrt-s}
  \dpar{\C{E}(n)}{s_i(n)}{} = -e_i(n).
\end{align}
%
Por la \iflatexml{}Ecuación~\ref{e2:neuron-general}
\else\autoref{e2:neuron-general}\fi, se tiene
%
\begin{align}\label{e2:deriv-s-wrt-v}
  \dpar{s_i(n)}{v_i(n)}{} \tab = f'(v_i(n)),\tabs
  \dpar{v_i(n)}{w_{ij}(n)}{} \tab = s_j(n).\\
\end{align}
%
Reemplazando las Ecuaciones
\ref{e2:deriv-E-wrt-s}--\ref{e2:deriv-s-wrt-v} en la
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w-expanded}
\else\autoref{e2:deriv-E-wrt-w-expanded}\fi, se tiene
%
\begin{align}\label{e2:deriv-E-wrt-w}
  \dpar{\C{E}(n)}{w_{ij}}{} = -e_i(n) f'(v_i(n)) s_j(n).
\end{align}
%
Cuando la neurona $i$ está en la capa de salida, esta fórmula se
utiliza directamente para el cálculo de $\dpar{\C{E}(n)}{w_{ij}}{}$,
ya que la señal de error $e_i(n)$ de la neurona viene dada por la
diferencia entre el valor deseado $y_k(n)$ y la salida de la neurona
$s_i(n)$ según la según la
\iflatexml{}Ecuación~\ref{e2:error-energy-neuron}
\else\autoref{e2:error-energy-neuron}\fi.

Sin embargo, las neuronas en las capas ocultas también deberán ajustar
sus pesos, ya que su valor de salida tiene influencia en la salida
global de la red, aunque resulta imposible determinar una ``respuesta
deseada'' correspondiente $y_k(n)$.  Por ello, cuando la neurona $i$
se ubica en una capa oculta, el cálculo de $\dpar{\C{E}(n)}{w_{ij}}{}$
resulta más complejo.  En particular, se desea obtener el valor de
$\dpar{\C{E}(n)}{s_i}{}$ correspondiente a una neurona en la capa
oculta.

El razonamiento para el cálculo de $\dpar{\C{E}(n)}{s_{i}}{}$, cuando
la neurona $i$ es oculta, es como sigue. Considérese en primer lugar
que el valor de $\dpar{\C{E}(n)}{s_k}{}$ es conocido, con
$k\in{}\C{S}(i)$, siendo $\C{S}(i)$ el conjunto de índices de todas
las neuronas en la capa siguiente a la capa de $i$.  Entonces, se
puede ver que
%
\begin{align}\label{e2:deriv-E-wrt-s-hid} %mlp5
  \dpar{\C{E}}{s_i}{} \tab= \sum_{k\in\C{S}(i)}
      \dpar{\C{E}}{s_k}{}\dpar{s_k}{s_i}{} \notag\\
    \tab= \sum_{k\in\C{S}(i)}
      \dpar{\C{E}}{s_k}{}\dpar{s_k}{v_k}{}\dpar{v_k}{s_i}{} \notag\\
    \tab= \sum_{k\in\C{S}(i)} \dpar{\C{E}}{s_k}{} s'(v_k) w_{ki}.
\end{align}
%
Reemplazando la
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-s-hid}\else\autoref{e2:deriv-E-wrt-s-hid}\fi
y la
\iflatexml{}Ecuación~\ref{e2:deriv-s-wrt-v}\else\autoref{e2:deriv-s-wrt-v}\fi
en la 
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w-expanded}\else\autoref{e2:deriv-E-wrt-w-expanded}\fi,
se tiene entonces
%
\begin{align}\label{e2:deriv-E-wrt-w-hid} %mlp5
  \dpar{\C{E}(n)}{w_{ij}}{} =
  \sum_{k\in\C{S}(i)} \dpar{\C{E}(n)}{s_k(n)}{} s'(v_k(n)) w_{ki}
  f'(v_i(n)) s_j(n).
  %% \dpar{\C{E}}{s_i}{} \tab= \sum_{k\in\C{S}(i)}
  %%     \dpar{\C{E}}{s_k}{}\dpar{s_k}{s_i}{} \notag\\
  %%   \tab= \sum_{k\in\C{S}(i)}
  %%     \dpar{\C{E}}{s_k}{}\dpar{s_k}{v_k}{}\dpar{v_k}{s_i}{} \notag\\
  %%   \tab= \sum_{k\in\C{S}(i)} \dpar{\C{E}}{s_k}{} s'(v_k) w_{ki}.
\end{align}
%
Esta fórmula es la clave del algoritmo de retropropagación: por la
arquitectura del MLP, la salida $s_i(n)$ de una neurona oculta ejerce
una influencia en la salida de todas las neuronas en las capas
subsiguientes.  Del mismo modo, se tiene que el \e{gradiente local} de
la señal de error de la neurona viene determinado por los gradientes
de error de todas las neuronas en las capas subsiguientes.

Comenzando el cálculo de $\dpar{\C{E}(n)}{w_{ij}}{}$ por la capa de
salida
(\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w}\else\autoref{e2:deriv-E-wrt-w}\fi)
y continuando con las capas ocultas en orden inverso
(\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w-hid}\else\autoref{e2:deriv-E-wrt-w-hid}\fi),
el gradiente resulta calculable para todas las neuronas.  Por ello el
nombre `` retropropagación'': se dice que la información del gradiente
de error se propaga hacia atrás a partir de la capa de salida hasta la
primer capa oculta.

El desarrollo del algoritmo de retropropagación a mediados de los años
80 representó un hito en el campo de las redes neuronales, ya
que brindó un método computacionalemnte eficiente, usable, para
entrenar el perceptrón multicapa.
%
\subsubsection{Ajuste de los pesos de la red}
%
En cada paso del entrenamiento $n$, una vez calculada la salida de la
red ante la netrada $\xx(n)$ y el gradiente
$\dpar{\C{E}(n)}{w_{ij}}{}$ para todos los pesos de la red, el
algoritmo aplica una corrección $\Delta{}w_{ij}(n)$ sobre el peso
$w_{ij}(n)$ que viene dada por la ``regla delta''
%
\begin{align}\label{e2:delta-rule}
  \Delta w_{ij}(n)\tab=-\eta *\dpar{\C{E}(n)}{w_{ij}}{},
\end{align}
%
en donde $\eta$ es el parámetro \e{velocidad de aprendizaje} del
algoritmo de retropropagación. La utilización del signo menos en
(\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w}\else\autoref{e2:deriv-E-wrt-w}\fi)
indica el \e{descenso por gradiente} en el espacio de los pesos
$w_{ij}$ de la red, esto es, el ajuste efectuado busca un cambio en la
dirección que reduce el valor de $\C{E}(n)$.
%
\subsubsection{Entrenamiento por época}
%
El traslado del entrenamiento del perceptrón multicapa a una estrategia
de ajuste por época deriva de manera sencilla de la definición de
la energía de error promedio $\C{E}_{\T{av}}$ para la época 
(\iflatexml{}Ecuación~\ref{e2:average-energy-net}\else\autoref{e2:average-energy-net}\fi):
una vez calculadas las salidas $s(n)$ y los gradientes de error instantáneo
$\dpar{\C{E}(n)}{w_{ij}}{}$ para cada patrón $n$ en el conjunto de entrenamiento,
el gradiente de error promedio para la época viene dado por
%
\begin{align}
  \dpar{\C{E}_{\T{av}}(\ell)}{w_{ij}}{}\tab=\frac{1}{\ell}\sum_{n=1}^\ell\dpar{\C{E}(n)}{w_{ij}}{},
\end{align}
%
y la corrección a aplicar a los pesos viene dada por
%
\begin{align}\label{e2:delta-rule}
  \Delta w_{ij}(n)\tab=-\eta *\dpar{\C{E}_{\T{av}}(\ell)}{w_{ij}}{}.
\end{align}
%

















%
Si bien la regla básica de aprendizaje es simple, con frecuencia
resulta difícil elegir en forma adecuada la velocidad de
aprendizaje. Una buena elección depende de la forma de la función de
error, que obviamente cambia con cada problema de aprendizaje
planteado, además de ser una función de dimensionalidad elevada.  Una
velocidad de aprendizaje pequeña resultará en un tiempo elevado de
convergencia en una función de error ``chata'', mientras que una velocidad
de aprendizaje grande posiblemente genere oscilaciones, previniendo
que el error caiga por debajo de un cierto valor.  Más aún, si bien la
convergencia a un mínimo local puede probarse en determinadas
circunstancias, no existen garantías que el algoritmo encuentre un
mínimo global de la función de error.

Otro problema con el descenso por gradiente es la influencia
``contraintuitiva'' de la derivada parcial sobre el tamaño del paso en
el ajuste de los pesos: si la función de error es ``playa'', la
derivada es relativamente pequeña, resultando en un paso pequeño.  Por
otro lado, ante la presencia de pendientes bruscas en la función de
la energía, en donde se deberían tomar pesos pequeños dada la
elevada pendiente, las derivadas ``grandes'' llevan a pasos grandes,
llevando posiblemente la búsqueda a una región completamente diferente
del espacio de los pesos.
%
\subsubsection{Término de momento}
%
Una idea para hacer el entrenamiento más estable es la adición de un
\e{término de momento}
%
\begin{align}\label{mlp8}
  \Delta w_{ij}(t) = - \epsilon \dpar{E}{w_{ij}}{}(t)
    + \mu \Delta w_{ij}(t-1).
\end{align}
%
El parámetro de momento $\mu$ escala la influencia del paso anterior
de actualización de los pesos en el paso actual. Esta técnica funciona
bien en muchas tareas de aprendizaje, aunque no significa un
incremento general de la estabilidad o de la velocidad de
convergencia.  A veces, el descenso de gradiente sin término de
momento obtiene iguales o mejores resultados que con el término de
momento. Usualmente, al utilizar aprendizaje con término de momento se
deberá disminuir la velocidad de aprendizaje para evitar inestabilidad
en el proceso de aprendizaje.

