%
%
\subsection{SVM de margen blando}
%
La máquina de vectores de soporte llamada ``de margen blando''
incorpora regularización al proceso de entrenamiento, permitiendo la
clasificación errónea de ciertos ejemplos de entrenamiento.  Al
aplicar regularización, los modelos resultantes son más simples, y
además, permite entrenar el clasificador con datos de entrenamiento
que no son linealmente separables.

En la máquina de vectores de soporte sin regularización (``margen
duro''), el objetivo del entrenamiento es reducir la complejidad del
modelo resultante: esto puede verse a partir de la interpretación
geométrica de $\ww$: una gran norma de $w$ corresponde a un pequeño
margen geométrico $\rho=1/\|\ww\|$, y consecuentemente a un modelo más
complejo.  La SVM de margen blando agrega al objetivo de minimización
una medida del error de clasificación sobre el conjunto de
entrenamiento.  Para cada ejemplo de entrenamiento $(\xx_i,y_i)$ se
introduce la variable de holgura no negativa $\xi_i$, que es la
función de pérdida ``bisagra''
%
\begin{align}
  \xi_n = \max\{0,1-y_n(\langle{}\ww,\Phi(\xx_i)\rangle+b)\}.
\end{align}
%
La variable $\xi_i$ mide la violación funcional del margen para cada
ejemplo $(\xx_i,y_i)$, que es la distancia al hiperplano de separación
para aquellos ejemplos incorrectamente clasificados.

Se introduce un parámetro de regularización $C>0$ que ajusta el
balance entre complejidad del modelo y la cantidad de error, y se
formula el problema de optimización \emph{primal}
%
\begin{align}
  \label{svm-primal-blando}
  \begin{split}
    \min_{(\ww,b,\xi)} \quad & \frac{1}{2}\|\ww\|^2+C\sum_{i=1}^{\ell}\xi_i\\
    \T{sujeto a} \quad &
    y_i\left(\langle{}\ww,\BPhi(\xx_i)\rangle+b\right) \geq 1-\xi_i, i=1,\ldots,\ell, \\
    & \xi_i \geq 0, i=1,\ldots,\ell.
  \end{split}
\end{align}
%

%% La SVM de margen blando (con regularización ``L1'' o ``de norma 1'')
%% es el tipo de clasificador SVM más utilizado.  Consiste en un
%% clasificador de margen máximo con el truco del núcleo, tal como la SVM
%% de margen duro. A diferencia de ésta, sin embargo, la SVM de margen
%% blando contiene un término de regularización que controla la
%% ``cantidad de error'' permitido al modelo, obteniendo un balance entre
%% el error de entrenamiento y la complejidad del modelo obtenido.  La
%% introducción del término de regularización a la SVM de margen rígido
%% permite su aplicación a problemas no separables en el espacio imagen,
%% ya que se permiten errores de clasificación en el entrenamiento.

%% Como medida de complejidad del modelo
%% $h(\xx)=\T{signo}(\langle{}\ww,\BPhi(\xx)\rangle{}+b)$
%% se utiliza el valor $\frac{1}{2}\|\ww\|^2$, que es la misma función optimizada en
%% la SVM de margen rígido.

%% Otra forma de ver este concepto es que para valores pequeños de
%% $\|w\|$, la función $\langle{}\ww,\BPhi(\xx)\rangle+b$ es suave en el
%% espacio de entrada, y para valores mayores de $\|w\|$ se vuelve más
%% ``abrupta'' (en el sentido que posee derivadas más grandes).

%% Como medida del error de entrenamiento, se utiliza una función de
%% pérdida $L:X\times\RR\times\RR\rightarrow\RR^{\geq0}$. 


Para obtener la forma dual de este problema, en primer lugar se
calcula el funcional lagrangiano
%
\begin{align}
  \begin{split}    
  \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta) &= \frac{1}{2} \|\ww\|^2 + C \sum_{i=1}^{\ell} \xi_i \\
  &+ \sum_{i=1}^{\ell} \alpha_i (1-\xi_i-y_i(\langle\ww,\BPhi(\xx_i)\rangle+b)) - \sum_{i=1}^{\ell} \beta_i\xi_i
  \end{split}
\end{align}
%
En el mínimo, se establecen las derivadas respecto de las variables
$\ww$, $b$, $\Bxi$ a cero y se obtiene
%
\begin{align}
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{\ww}{}
    &=\ww-\sum_{i=1}^\ell y_i\alpha_i\BPhi(\xx_i) = 0
    &\Rightarrow \ww = \sum_{i=1}^\ell y_i\alpha_i\BPhi(\xx_i)
  \\
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{b}{}
    &=-\sum_{i=1}^\ell y_i\alpha_i = 0
    &\Rightarrow \sum_{i=1}^\ell y_i\alpha_i = \yy^T\Balpha = 0,\\  
  \dpar{ \C{L}(\ww,b,\Bxi,\Balpha,\Bbeta)}{\Bxi_i}{}
    &=C-\alpha_i-\beta_i = 0
    &\Rightarrow \alpha_i \leq C,
\end{align}
%
Reescribiendo estos resultados en el problema dual, se tiene
%
\begin{align}
\begin{split}\label{svmprob-dual-soft}
    \max_{\Balpha} \quad
    & f(\Balpha) = \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha\\
    \T{sujeto a}\quad & \yy^T\Balpha = 0, \\
                      & 0\leq\alpha_i\leq C, \T{ para todo } i\in {1,\ldots,\ell }.
\end{split}\end{align}
%
Este problema es el mismo que aquel de la SVM de magen duro, salvo por la restricción
$\alpha_i\leq C$.

El problema \ref{PROBLEMA-SVM} es el tipo de clasificador SVM más utilizado,
y se denomina también SVM ``de norma 1'' o ``con regularización L1''. Esto
se debe a que la variable holgura $\xi_i$ se incorpora con potencia 1.
Una variante de la SVM de margen suave utiliza la norma dos del
vector holgura $\B{\xi}=(\xi_1,\ldots,\xi_\ell)^T$, correspondiente
a la pérdida de bisagra al cuadrado. En este caso, el objetivo primal
se define como
%
\begin{align}
  \label{svm-l2}
  \frac{1}{2}\|w\|^2+\frac{C}{2}\sum_{n=1}^{\ell}\xi_n^2
\end{align}
%
con las mismas restricciones que el problema anterior.
Esta formulación se denomina ``SVM de margen blando con regularización L2''.
