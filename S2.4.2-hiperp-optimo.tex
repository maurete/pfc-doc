%
%
\subsection{El hiperplano de separación óptimo}
%
Dado un conjunto de entrenamiento
$D=\left((\xx_1,y_1),\ldots,(\xx_\ell,y_\ell)\right)$, se dice que
éste es linealmente separable en el espacio imagen $Z$ cuando existe
una función discriminante lineal $f(\xx)=\ww^T\BPhi(\xx)+b$ tal que su
signo permite determinar, sin errores, la clase de todos los elementos
del conjunto.
Cuando el conjunto $D$ es linealmente separable en $Z$, se sabe que
existen infinitas funciones discriminantes lineales capaces de
clasificar correctamente todos los elementos, y por ende, existen
infinitos hiperplanos de separación de clases.

La característica saliente de la máquina de vectores de soporte por
sobre el clasificador lineal básico es que, durante el entrenamiento,
ésta trata de encontrar un hiperplano de separación $\ww^T\zz+b$
\e{óptimo}, que maximiza la distancia a los puntos
$\zz_i=\BPhi(\xx_i)$ más cercanos. Esto se expresa a través del
problema de optimización
%
\begin{align}\label{e2:svm-problem-basic}
  \begin{split}
    \min_{\ww,b} \quad\tabs \frac{1}{2}\|\ww\|^2, \\
    \T{sujeto a}\quad\tabs y_i f_i\geq1,\,i=1,\ldots,\ell\,.
  \end{split}
\end{align}
%
Aquí, $f_i=f(x_i)=\ww^T\BPhi(\xx_i)+b$ es la función discriminante
lineal aplicada al vector $\xx_i$. En términos coloquiales, este
problema puede leerse
%
\begin{align*}
  \begin{split}
    \min \frac{1}{2}\|\ww\|^2 \quad \tabs \T{encontrar el $\ww$ más
      pequeño según la norma euclídea $\|\cdot\|$}\ldots\\
    |f_i|\geq1 \quad \tabs \T{estableciendo en $1$ la distancia del
      hiperplano a los puntos más cercanos},\\
    y_i\hat{y}_i>0\quad \tabs \T{y tal que la clase $y_i$ coincida con el
      signo del discriminante ($\hat{y}_i=\T{signo}(f_i)$)}.
  \end{split}
\end{align*}
%
La distancia de un vector $\zz_i=\BPhi(\xx_i)$ al hiperplano
$\ww^T\zz+b=0$ viene dada por $d_i=\frac{\ww}{\|w\|}\cdot\zz_i+b$, por
ello, minimizar $\|w\|$ equivale a maximizar la distancia $d_i$.

Puede demostrarse que el vector solución $\ww_*$ al problema
(\ref{e2:svm-problem-basic}) es una combinación lineal de aquellos
vectores $\zz_i$ para los cuales $|d_i|=1$. Estos $\zz_i$ se denominan
\e{vectores de soporte}, y los hiperplanos $\ww^T\zz+b=\pm1$ que
contienen los vectores de soporte conforman el \e{margen} del
clasificador.

%% Desde el punto de vista numérico, el problema
%% (\ref{e2:svm-problem-basic}) es un problema de optimización cuadrático
%% convexo con restricciones lineales.
