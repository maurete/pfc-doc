%
%
\subsection{Arquitectura}
%
El perceptrón multicapa, así como las redes neuronales en general, es
una arquitectura de cómputo paralelo en la que la unidad de cómputo
básica es la neurona (artifical). Desde un punto de vista matemático,
la red se representa como un grafo orientados, donde los nodos son
neuronas y las aristas, con un sentido y una ponderación (peso)
asociado, conectan las salidas de una neurona con la entrada de otra.

Según la topología del grafo, se dice que una red neuronal es
\e{acíclicas} o \e{con propagación hacia adelante} cuando no existen
bucles dentro de la red. Similarmente, se dice que una red es
\e{recurrente}, cuando contiene bucles de {retroalimentación} en su
topología. En una red recurrente, resulta posible encontrar al menos
un camino que conduce al mismo punto de partida.

%% y conectadas por ``enlaces''
%% denominados \e{pesos sinápticos}
%% con una arquitectura organizada en capas.
%% Cada nodo se conecta mediante enlaces ponderados denominados
%% ``pesos sinápticos'' a todos los nodos en las capas adyacentes.

%% La primer capa, denominada capa de entrada, contiene nodos sensores
%% que ``leen'' un vector de entrada externo, y lo transmiten a través
%% de los enlaces sinápticos a la primer capa oculta.
%% Las capas subsiguientes se componen de ``neuronas'', nodos computadores
%% que determinan su valor de salida a partir de las salidas de los nodos en
%% la capa anterior y los valores de los pesos sinápticos que los conectan.

%% Las capas ocultas reciben como entrada las
%% salidas de la capa anterior, determinan su valor de salida aplicando
%% una \e{función de activación} a la suma ponderada sus entradas, y
%% transmiten su valor de salida a la capa siguiente. Este proceso
%% continúa capa por capa hasta alcanzar la capa de salida.

El perceptrón multicapa tiene una arquitectura acíclica organizada en
\e{capas}. La primer capa se denomina \e{capa de entrada} y contiene,
en lugar de neuronas, nodos sensores que ``leen'' el vector presentado
como entrada a la red, estableciendos su valores de salida al valor de
los componentes del vector.  Las capas subsiguientes contienen
neuronas, cada una de las cuales recibe como entrada \e{todas} las
salidas de la capa anterior.  Para el cálculo de las salidas de cada
capa se requiere conocer los valores de salida de la capa anterior,
por lo que se dice que el vector de entrada se \e{propaga hacia
  adelante} a través de la red.

%% Dentro de una misma capa, las neuronas no se conectan entre sí.

La salida de la red en su conjunto es aquella de las neuronas en la
última capa, denominada \e{capa de salida}. Las capas que no son ni de
entrada ni de salida se denominan \e{capas ocultas}.

En la \iflatexml{}Figura~\ref{fig:mlp}\else\autoref{fig:mlp}\fi se
representa un perceptrón multicapa de 2 capas, que lee un vector de 3
elementos, el cual es propagado a través de la capa oculta de 4
neuronas para finalmente hasta alcanzar las 2 neuronas en la capa de
salida.
