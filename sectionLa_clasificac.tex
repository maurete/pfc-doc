\section{La clasificación como forma de aprendizaje supervisado}
En el contexto de la Inteligencia Computacional, la clasificación es
un tipo de \e{aprendizaje supervisado}, en el cual el objetivo es
\e{inferir} una función que relaciona \e{datos de entrada} con las
respectivas \e{salidas deseadas} a partir de una serie de ejemplos que
conforman el llamado \e{conjunto de entrenamiento}.  Un algoritmo que
implementa la clasificación se conoce como un \e{clasificador}. La
función matemática que transforma datos de entrada a una categoría
(clase) se llama \e{modelo} (o \e{hipótesis}).

Un clasificador es un tipo de \e{máquina de aprendizaje}, definida
como un algoritmo que analiza los datos del conjunto de entrenamiento
y produce una función de correspondencia (el modelo) que puede ser
usada para relacionar nuevos datos de entrada a un valor de salida.
De este modo, el clasificador es capaz de replicar el comportamiento
del sistema o proceso que pretende modelar, y se dice que el modelo
obtenido es capaz de \e{generalizar} a partir de los datos de
entrenamiento.

En lo que sigue de esta sección se presenta una introducción a los
conceptos básicos del aprendizaje supervisado, basada
en la discusión presentada por \citeauthor{glasmachers} en
\cite{glasmachers}.

\subsubsection{Aprendizaje supervisado}
Formalmente, el aprendizaje supervisado se define como sigue: sean $X$
e $Y$ espacios vectoriales, y sea $(\xx,\yy)\in{}X\times{}Y$ un
\e{ejemplo} (correspondiente a una observación) que relaciona una
entrada $\xx\in{}X$ con la salida correspondiente $\yy\in{}Y$.

Se asume que existe una distribución de probabilidades fija y
desconcida $\nu$ sobre el producto $X\times Y$, llamada
\e{distribución generadora} de los datos.  El aprendizaje supervisado
trata la extracción de propiedades de esta distribución a partir de un
número finito de variables aleatorias, denominadas \e{ejemplos}, que
conforman el conjunto de datos de entrenamiento

\begin{align}
  D = \left((\B{x}_1,\yy_1),\ldots,(\B{x}_\ell,\yy_\ell)\right) \in
  \mathcal{D}=\bigcup_{\ell=1}^\infty (X\times Y)^\ell,
\end{align}
que se considera como una muestra independiente e idénticamente
distribuida de $\nu$, esto es, $D\sim\nu$.

\subsubsection{Modelo}
La extracción de propiedades de $\nu$ se efectúa mediante un modelo
$h:X\rightarrow{}Y$. En un problema de clasificación, $Y$ es un
conjunto discreto y finito, donde cada valor posible de $Y$ se
corresponde con una \emph{clase}. El caso más básico e importante es
la \e{clasificación binaria}, en la que se consideran sólo dos valores
en $Y$, comúnmente $+1$ y $-1$.

\subsubsection{Máquina de aprendizaje}
Sea $\C{H}=\{h:X\rightarrow Y\}$ el conjunto de todos los modelos
posibles. Una máquina de aprendizaje es una transformación

\begin{align}
  A:\mathcal{D}\rightarrow\C{H}
  \label{eq:maqaprendizaje}
\end{align}
que relaciona un conjunto de datos a un modelo.
La máquina de aprendizaje $A$ no es necesariamente una transformación
en sentido matemático, sino que en general consiste en un algoritmo
que efectúa el proceso denominado \e{entrenamiento}.

\subsubsection{Clasificador}
\hl{redundante, lo dejo para reafirmar el concepto}

Un clasificador es una máquina de aprendizaje para la cual las salidas
posibles del modelo es un conjunto finito de valores discretos.  En
adelante, siempre que se hable de máquinas de aprendizaje se incluyen
los clasificadores dentro de esta denominación.

\subsubsection{Función de pérdida}
Para un ejemplo $(\xx,\yy)\in{}X\times{}Y$, la función de pérdida mide
la cantidad de error del modelo $\hat{\yy}=h(\xx)$ mediante la
transformación $L(\xx,\yy,\hat{\yy})$.  Si la predicción resultante
del modelo $\hat{\yy}$ es correcta, el error es nulo. En la práctica,
la mayoría de las funciones de pérdida no dependen de la entrada
$\xx$, sino sólo de la predicción $\hat{\yy}$ y la clase $\yy$.

Algunas de las funciones de pérdida más comunes son:

\begin{align}
  \T{\e{Pérdida 0-1}:} &\qquad
    L(\yy,\hat{\yy})=\begin{cases}0,\quad{}\yy=\hat{\yy}\\
      1,\quad\T{en otro caso}\end{cases} \\
  \T{\e{Pérdida ``bisagra''}:} &\qquad
    L(\yy,\hat{\yy})=\max\{0,\yy-\hat{\yy}\}\\
  \T{\e{Error absoluto}:} &\qquad
    L(\yy,\hat{\yy})=|\yy-\hat{\yy}|\\
  \T{\e{Error cuadrático}:} &\qquad
    L(\yy,\hat{\yy})=(\yy-\hat{\yy})^2.
\end{align}

\subsubsection{Error de generalización}
El error de generalización
$\mathcal{R}_\nu:\C{H}\rightarrow\RR^{\geq0}$ es el error esperado de
un modelo $h$ al clasificar un ``nuevo'' ejemplo, independiente
del conjunto $D$, y se define a partir de la función de pérdida $L$ según

\begin{align}
  \mathcal{R}_\nu=\mathds{E}_\nu\left[L(x,y,h(x))\right]
  =\int_{X\times Y} L(x,y,h(x))\, d\nu(x,y).
\end{align}
El cálculo de este error requiere conocer la distribución generadora
de datos $\nu$, sin embargo, toda la información que se tiene de $\nu$
es la realización $D$, para la cual no se conoce su probabilidad.
Por ello, la aplicación del concepto de error de generalización es
teórica, utilizando en la práctica medidas que lo aproximan.

\subsection{Entrenamiento}
Todo el conocimiento que se tiene sobre la distribución generadora de
datos $\nu$ proviene del conjunto de datos finito
$D=\left((x_1,y_1),\ldots,(x_\ell,y_\ell)\right)$.  Este conjunto de
datos define una distribución empírica

\begin{align}
  \hat{\nu}=\frac{1}{\ell}\sum_{i=1}^{\ell}\delta_{(x_i,y_i)}(x,y)
\end{align}
donde $\delta_{(x,y)}$ denota el delta de Dirac en
$(x,y)\in{}X\times{}Y$.  Esta distribución conduce a la definición del
\e{error de entrenamiento} como el error de un modelo $h$ sobre los
datos de entrenamiento en $D$:

\begin{align}
  \hat{\C{R}}_D(h)=E_\T{entrenamiento}=\R{E}_{\hat{\nu}}[L(x,y,h(x))]
  =\frac{1}{\ell}\sum_{i=1}^\ell L(x_i, y_i, h(x_i)),
\end{align}
en donde $L$ es una función de pérdida.
El entrenamiento de la máquina de aprendizaje consiste en
seleccionar el modelo con el mínimo error de entrenamiento

\begin{align}
  \hat{h}:=\arg \min \left\{ \hat{\C{R}}_D(h)\middle|h \in H\right\}.
\end{align}
Este procedimiento se conoce también como la minimización del riesgo
empírico, y define una máquina de aprendizaje
$A_H:\C{D}\rightarrow{}H$ para cada clase $H$ de modelo.

\subsubsection{Sobreajuste}
La minimización del error de entrenamiento

Se dice que un modelo presenta \e{sobreajuste} cuando éste describe
el conjunto de datos de entrenamiento en lugar de la relación
subyacente $\nu$ entre las variables de entrada y de salida.
Intuitivamente, se puede decir que existe sobreajuste cuando el modelo
``memoriza'' ejemplos del conjunto de aprendizaje en lugar de
``aprender'' la relación subyacente entre los datos.  El problema del
sobreajuste existe debido a que la máquina de aprendizaje no recibe
información acerca de la \e{probabilidad} de cada ejemplo observado,
ignorando el ruido y los efectos aleatorios en el conjunto de datos de
entrenamiento.

\subsection{Estimación del error de generalización}
El error de generalización es la medida más acertada para evaluar la
aptitud de un modelo al clasificar datos nuevos.  Sin embargo, al
tratarse de un valor no calculable se hace necesario estimarlo
mediante medidas de error alternativas.

\subsubsection{Error de prueba}
Una situación óptima se da cuando se dispone de una muestra
independiente e idénticamente distribuida a $\nu$ denominada
\e{conjunto de prueba}:

\begin{align}
  T=((\tilde{x}_1,\tilde{y}_1),\ldots,(\tilde{x}_N,\tilde{y}_N))\sim\nu^N,
  \label{eq:conj-prueba}
\end{align}
para el cual se define el \e{error de prueba} (o \e{de test}) como la
pérdida media

\begin{align}
  E_{test}=\frac{1}{N}\sum_{n=1}^N
  L(\tilde{x}_N,\tilde{y}_N,h(\tilde{x}_N)).
  \label{eq:error-prueba}
\end{align}
Este error de prueba es una estimación no sesgada del error de
generalización verdadero $\C{R}_\nu(h)$, y puede ser utilzado para
comparar modelos.

\subsubsection{Método de retención}
\label{retención}

En la práctica, la mayoría de las veces el conjunto de prueba $T$ no
viene dado como tal, y en cambio se genera un conjunto $T^*\subset{}D$
seleccionando ejemplos al azar del conjunto de entrenamiento $D$.  A
fin de que exista una cierta ``independencia''\footnote{Estrictamente,
  la independencia no existe en este caso, ya que la pertenencia de un
  ejemplo al conjunto de entrenamiento $D^*$ implica la no pertenecia
  al conjunto de prueba $T^*$.} de los datos en $T^*$ respecto del
conjunto de entrenamiento, se genera un nuevo conjunto de entrenamiento
$D^*\subset{}D:D^*\cap{}T^*=\emptyset$ que será usado para entrenar la
máquina de aprendizaje. El conjunto $D^*$ se denomina en ocasiones
\e{conjunto de estimación}.
Finalmente, el error de prueba puede ser estimado calculando el error
sobre el conjunto $T^*$. Este método es conocido como el \e{método de
  retención}.

\subsubsection{Validación cruzada de $k$ iteraciones}
En un procedimiento de \emph{validación cruzada de $k$ iteraciones}
\cite{crossval}, el conjunto $D$ se subdivide en $k$ conjuntos
disjuntos $D_1,\ldots,D_k$ de tamaños similares, denominados \e{particiones}.

El entrenamiento de la máquina de aprendizaje se efectúa sobre $k-1$
particiones, y se clasifican aquellos datos de la partición restante
(llamada \e{de validación}) para obtener una medida de desempeño de
clasificación de la partición actual. Se repite el procedimiento $k$
veces, seleccionando en cada iteración una partición diferente para
validación.  Una vez completado el proceso, se dispondrá de $k$
medidas de desempeño de la máquina de aprendizaje. Estas medidas
pueden ser promediadas para obtener un estimador del desempeño de la
máquina de aprendizaje. Este estimador es una medida más acertada que
el error de entrenamiento para estimar el error de generalización del
modelo.

El error de validación cruzada presenta una menor varianza que aquel
del método de retención, sin embargo, al no ser los subconjuntos $D_i$
independientes e idénticamente distribuidos entre sí, se introduce un
sesgo en la estimación del error de generalización.  Tal como en el
caso del método de retención, en conjuntos de datos pequeños el método
de validación cruzada presenta una alta varianza respecto de la
elección de diferentes $D_i$.

\subsubsection{Validación cruzada dejando uno fuera}
En el método de validación cruzada ``clásico'' de $k$ iteraciones, se
tiene para valores pequeños de $k$ una reducción en la varianza y para
valores grandes una reducción del sesgo introducido por el particionado.
En el extremo, cuando se
establece $k$ al número de elementos en $D$, se está ante un método de
\e{validación cruzada dejando uno fuera}. La condición $k=l=|D|$
implica la reducción del sesgo a un valor mínimo, ya que se separa
sólo un elemento para validación. Sin embargo, la varianza obtenida
mediante este método es máxima.

Para obtener una estimación del error de generalización mediante este
método es necesario entrenar el clasificador $l$ veces, lo que deriva
en un costo computacional muy elevado. Sin embargo, para el caso de un
clasificador SVM existen técnicas para el cálculo eficiente del error
dejando uno fuera \cite{chapelle, lee-keerthi}.

\subsection{Regularización}
Al minimizar el error de entrenamiento se produce una tendencia al
sobreajuste, lo que se traduce en un rendimiento de generalización
subóptimo. Este sobreajuste genera fronteras de decisión relativamente
complejas para problemas que en realidad tienden a ser más simples.
Esto se condice con un principio a menudo referido como la \e{navaja de
Ockam}:
``las entidades no deben multiplicarse más allá de lo necesario'',
afirmando que las explicaciones simples son más plausibles y los
modelos sencillos más explicativos y más confiables que los
complicados.

La regularización consiste en forzar la simplicidad del modelo, y es
una de las estrategias principales utilizadas para evitar el
sobreajuste del modelo generado por la máquina de aprendizaje. Existen
dos formas comunes para incorporar simplicidad en el modelo:

\begin{itemize}
\item Limitar el modelo a una subclase de funciones simples.
\item Añadir un término de penalización por complejidad al objetivo
  del error de entrenamiento.
\end{itemize}
El segundo enfoque requiere que podamos medir la ``complejidad'' del
modelo. Ambos procedimientos regularizan el proceso de aprendizaje, ya
que favorecen las hipótesis más simples.

Cuando el entrenamiento de la máquina de aprendizaje es un proceso
gradual, una técnica alternativa de regularización consiste en
abortar el proceso de entrenamiento al momento que se detecta
que el error de generalización empieza a incrementarse.

Se debe notar que la regularización en general entra en conflicto con
la minimización del error de entrenamiento, y un modelo regularizado
cometerá más errores sobre el conjunto de entrenamiento que un modelo
no regularizado.  Sin embargo, se ha de esperar que el modelo
regularizado tenga mayor capacidad de generalización.

\subsection{Medidas de evaluación de un clasificador binario}









Los clasificadores binarios generan modelos cuya salida toma
exactamente dos valores posibles, que se interpretan en forma genérica
como ``verdadero'' y ``falso''. Para esta clase especialmente
importante de clasificadores, se definen una serie de métricas que
relacionan la pertenencia de clase de los datos de entrada con el
error de clasificación.

Dado un modelo $h$ con salida binaria, se definen las siguientes
medidas que caracterizan el resultado de clasificar un conjunto de
prueba $T$:

\begin{itemize}
[style=nextline]
  \item\e{Verdaderos positivos ($\VP$)}: número de elementos de clase
    positiva correctamente clasificados como positivos.
  \item\e{Verdaderos negativos ($\VN$)}: número de elementos de clase
    negativa correctamente clasificados como negativos.
  \item\e{Falsos positivos ($\FP$)}: número de elementos de clase
    negativa erróneamente clasificados como positivos.
  \item\e{Falsos negativos ($\FN$)}: número de elementos de clase
    positiva erróneamente clasificados como negativos.
\end{itemize}
Estas cuatro medidas conforman la llamada \e{matriz de confusión}
característica del clasificador, y dependen en todos los casos del
número de elementos positivos y negativos presentes en el conjunto de
prueba.

\subsubsection{Sensibilidad y especificidad}
A partir de los valores de la matriz de confusión se definen las
medidas de \e{sensibilidad ($\SE$)} y \e{especificidad ($\SP$)}:

\begin{align*}
\SE & = \frac{\VP}{\VP+\FN}, & \SP & = \frac{\VN}{\VN+\FP}.
\end{align*}
La sensibilidad, también llamada a veces \e{razón de veraderos
  positivos} (\e{TPR}), indica la \e{tasa de acierto} al clasificar
elementos de clase positiva, y la especificidad (también \e{razón de
  veraderos negativos}) representa la tasa de acierto al clasificar
ejemplos de clase negativa.  Ambas medidas son independientes del
número de elementos presentes en el conjunto de prueba, aunque no de
la proporción entre elementos positivos y negativos.

La importancia de las medidas de sensibilidad y especificidad es que
pueden utilizarse para elegir, por ejemplo, el clasificador con menor
cantidad de falsos negativos ($\SP=1$), una característica deseable en
aplicaciones tales como detectores de patógenos en bancos de sangre y
detectores de explosivos en aeropuertos.

\subsubsection{Otras medidas de desempeño}
En ocasiones se requiere medir el funcionamiento de un clasificador a
partir de una única medida que caracterice la aptitud del clasificador
en general.  En estos casos, se puede utilizar simplemente el promedio
de la sensibilidad y la especificidad, sin embargo, resulta mucho más
común utilizar la media geométrica entre la sensibilidad y la
especificidad

\begin{align}
\GM & = \sqrt{\SE\cdot\SP}.
\end{align}
Otras medidas comúnmente utilizadas para evaluación de un clasificador
binario son

\begin{align}
%  \begin{split}
    \T{Precisión} && \PR &=\frac{\VP}{\VP+\FP},\\
    \T{Exactitud} && \AC &=\frac{\VP+\VN}{\VP+\FP+\VN+\FN},\\
    \T{Valor-F}   && \T{\textsl{F}} &=\frac{\PR\cdot{}\SE}{\PR+\SE}, \\
    \parbox{4cm}{Coeficiente de correlación de Matthews} &&
    \T{\textsl{MCC}} &= \frac{\VP\cdot{}\VN-\FP\cdot\FN
    }{\sqrt{(\VP+\FP)(\VP+\FN)(\VN+\FP)(\VN+\FN)}}.
%  \end{split}
\end{align}