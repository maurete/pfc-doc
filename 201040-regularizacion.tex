%
%
\subsection{Regularización}
%
Se dice que un modelo presenta sobreajuste cuando genera soluciones
relativamente complejas para problemas que en realidad tienden a ser
más simples.  Esto evoca un principio a menudo referido como la
\e{navaja de Ockam}: ``las entidades no deben multiplicarse más allá
de lo necesario'', afirmando que las explicaciones simples son más
plausibles y los modelos sencillos son más explicativos y más
confiables que los complicados.

La \e{regularización} consiste en forzar la simplicidad del modelo,
evitando el
sobreajuste del modelo generado por la máquina de aprendizaje. Existen
dos formas comunes para incorporar simplicidad en el modelo:
%
\begin{itemize}
\item Limitar el modelo a una subclase de funciones simples.
\item Añadir un término de penalización por complejidad al objetivo
  del error de entrenamiento.
\end{itemize}
%
Ambos procedimientos regularizan el proceso de aprendizaje, ya
que favorecen las hipótesis más simples.
El segundo enfoque requiere alguna forma de medir la ``complejidad'' del
modelo.

Cuando el entrenamiento de la máquina de aprendizaje es un proceso
gradual, una técnica alternativa de regularización consiste en
abortar el proceso de entrenamiento al momento que se detecta
que el error de generalización empieza a incrementarse.

Se debe notar que la regularización en general entra en conflicto con
la minimización del error de entrenamiento, y un modelo regularizado
cometerá más errores sobre el conjunto de entrenamiento que un modelo
no regularizado.  Sin embargo, se ha de esperar que el modelo
regularizado tenga mayor capacidad de generalización.
