%
%
\subsection{Minimización del error empírico}
%
La estrategia de minimización del error empírico, aplicable a la
máquina de vectores de soporte, optimiza una función objetivo
denominada \e{error empírico} basada en una interpretación
probabilística de la salida del modelo del clasificador SVM.

La función error empírico fue propuesta por \citeauthor{ayat} en
\cite{ayat} para la selección automática de los parámetros del núcleo.
En el presente trabajo, se incorpora el cálculo del gradiente de la
función respecto del hiperparámetro de regularización $C$
\cite{keerthi,glasmachers} tal como se implementa en \cite{shark}.  De
este modo, la estrategia de minimización del error empírico soporta la
selección automática de todos los hiperparámetros de la máquina de
vectores de soporte, incluyendo la regularización $C$.
%
\begin{quote}
  \sbs{Terminología.}
  El lector experto encontrará ambigua la denominación de la función
  ``error empírico''.  En la disciplina este nombre se utiliza como
  equivalente de ``error de entrenamiento''.  En este trabajo, se
  mantiene la denominación de los autores \cite{ayat}, diferenciando
  entre ``error de entrenamiento'' como la tasa de error del modelo
  sobre el conjunto de entrenamiento, y ``error empírico'', que se
  trata de una interpretación probabilística del error de validación
  cruzada sobre el conjunto de entrenamiento.
\end{quote}
%

La definición de la función error empírico se basa en el error del
clasificador sobre un conjunto de prueba $T$, el cual es un estimador
no sesgado del error de generalización. Considérese la definición del
error de prueba sobre un conjunto
$T=((\tilde{x}_k,\tilde{y}_k))\,n=1,\ldots,N$ de clase conocida
(\iflatexml{}Ecuación~\ref{eq:error-prueba}\else\autoref{eq:error-prueba}\fi).
Este error puede escribirse en la forma equivalente
%
\begin{align}
\label{e3:error-test-alt}
  E^T = \frac{1}{N}\sum_{k=1}^{N} H(-{y}_k {c}_k))
\end{align}
%
donde $c_k=h(\xx_k)$ es la predicción de clase del modelo SVM
entrenado, y $H(\cdot)$ es la función escalón de Heaviside. Fijando el
tipo de núcleo utilizado y los conjuntos de datos de entrenamiento y
validación, se puede considerar
%
\begin{align}
\label{e3:error-test-func-theta}  
  E^T = E^T(\Btheta),
\end{align}
%
por lo que minimizar $E^T(\Btheta)$ equivale a seleccionar los
hiperparámetros óptimos $\Btheta$ para el conjunto $T$.  La función
$E^T$ así definida es una función objetivo válida para la comparación
de modelos, sin embargo, por definición es una función discontinua, lo
que imposibilita la optimización eficiente mediante descenso de
gradiente.

Una interpretación alternativa del concepto de error, basada en la
teoría bayesiana, permite desarrollar una función objetivo con
características deseables de continuidad y derivabilidad.  Si $p_k$ es
la \e{probabilidad a posteriori} de que el ejemplo $\xx_k$ pertenezca
a la clase positiva
%
\begin{align}
  \label{e3:pk}
  p_k = p(x_k) = P(c_k=+1|x_k), % = z_k
\end{align}
%
se puede caracterizar la \e{probabilidad de error} $E_k$ cometido al
clasificar el ejemplo $\xx_k$ según
%
\begin{align}
\label{e3:Ek}
  E_k = P(c_k\neq y_k) = |t_k-{p}_k| =
  \begin{cases}
    {p}_k, & t_k=0\\ 1-{p}_k, & t_k = 1,
  \end{cases}
\end{align}
%
donde $t_k=\frac{y_k+1}{2}$ es un ``valor deseado'' calculado a partir
de la clase conocida $y_k$.  Entonces, la probabilidad de error para
el conjunto $T$ puede escribirse
%
\begin{align}
\label{Err1}
  E = \sum_{k=1}^{N} E_k.
\end{align}
%
La función $E$ especifica el denominado \e{error empírico} y es la
función objetivo a minimizar. Para su cálculo, sin embargo, se
requiere el conocimiento de la probabilidad $p_k$, información que el
modelo SVM no brinda, al tratarse de una salida binaria. Sin embargo,
resulta posible calcular una probabilidad estimada $\hat{p}_k$, que
permite su utilización en la práctica.

\subsubsection{Estimación de la probabilidad ${p}_k$}
El modelo $h$ de una máquina de vectores de soporte
(\autoref{eq:svm-model-hard}) puede escribirse

\begin{align*}
  h(\xx_k) = \T{signo}(f_k), \qquad\qquad
  f_k=\langle\ww,\BPhi(\xx_k)\rangle+b.
\end{align*}
Utilizando el valor de $f_k$, la salida del modelo
previa a aplicar la función signo, se puede calcular una probabilidad
estimada $\hat{p}_k$ según

\begin{align}
  \hat{p}_k=\frac{1}{1+e^{Af_k+B}} \qquad \approx p_k.
\end{align}
Los parámetros $A$ y $B$ se determinan optimizando el problema

\begin{align}
  \arg\min_{A,B} \quad & -\sum_{k=1}^{N} \log(E_k)
  \label{abproblem}
\end{align}
sobre el conjunto de prueba $T$. La optimización de este sub-problema
se realiza mediante una técnica de descenso de gradiente, ya que las
derivadas de $\hat{p}_k$ respecto de sus parámetros $A$ y $B$ son
fácilmente calculables:

\begin{align*}
  \dpar{\hat{p}_k}{A}{}&=-f_k e^{Af_k+B}\frac{1}{(1+e^{Af_k+B})^2}
  =-f_k\hat{p}_k(1-\hat{p}_k),&\\
  \dpar{\hat{p}_k}{B}{}&=    -e^{Af_k+B}\frac{1}{(1+e^{Af_k+B})^2}
  =-   \hat{p}_k(1-\hat{p}_k).
\end{align*}
Esta técnica para obtener un estimador $p_k$ de la probabiidad a
posteriori real fue presentada por \citeauthor{platt} en \cite{platt}.
La elección de una función logística como $\hat{p}_k$ se basa en la
presunción de que las salidas $f_k$ para las entradas $\{\xx_k\}$ de
clase positiva tienen una distribución gaussiana en $f$.

Una interpretación intuitiva de $\hat{p}_k$ es que, para una salida
$f_k$ de gran magnitud que se ubica lejos del hiperplano de separación
en el espacio inducido por el núcleo, el clasificador SVM tiene amplia
certeza de su decisión.  Asimismo, se tendrá que cuando el valor de
$f_k=0$, $x_k$ recae exactamente en el plano de separación y se
obtiene la peor predicción posible $\hat{p}_k=0.5$\footnote{La
  condición $\hat{p}_k=0.5$ cuando $x_k$ se ubica sobre el plano de
  separación implica $B=0$, sin embargo, se considera igualmente $B$
  como un valor a optimizar, respetando el método original
  \cite{platt}.} y, similarmente, cuanto más ancho sea el margen de
separación, más suave deberá ser la pendiente de $\hat{p}_k$.

\subsubsection{Cálculo del gradiente $\grad{E}$}
Sustituyendo la probabilidad real $p_k$ por el estimador
$\hat{p}_k$, y calculando el gradiente, se tiene

\begin{align}
\label{gradE}
  \grad{E} = \sum_{k=1}^{N} \grad{E_k} =
  \sum_{ i:t_i=0  } \grad{\hat{p}_i}
  - \sum_{ j:t_j=1  } \grad{\hat{p}_j}
  = \sum_{k=1}^{N} y_k \grad{\hat{p}_k}
\end{align}
Por la regla de composición de las derivadas, la derivada
$\grad{\hat{p}_k}=\dpar{\hat{p}_k}{\theta_j}{}$ respecto de un
hiperparámetro ${\theta_j}$ es

\begin{align}
  \dpar{\hat{p}_k}{\theta_j}{} =
  \dpar{\hat{p}_k}{f_k}{}\dpar{f_k}{\theta_j}{} =
  -A\hat{p}_k(1-\hat{p}_k)\dpar{f_k}{\theta_j}{}.
  \label{eq:deriv-pk-thetak}
\end{align}
Para la determinación de $\dpar{f_k}{\theta_j}{}$ se sigue el
procedimiento propuesto en \cite{keerthi,glasmachers} e implementado
en \cite{shark}.
En primer lugar, se observa que $f_k$ puede calcularse según

\begin{align}
  f_k = \langle w,\Phi(x_k)\rangle+b = \sum_{i=1}^l \alpha_i y_i k(x_i,x_k) + b
  \label{fk}
\end{align}
donde $k(\cdot,\cdot)$ es la función núcleo, y $\alpha_i, b$ son los
parámetros del modelo SVM entrenado $h$.

Según sea el valor de $\alpha_i$ correspondiente al vector de
entrenamiento $\xx_i$, se definen los conjuntos de índices

\begin{align}
  \label{unbounded-sv-set}
  u &= \left\{i\in\{1,\ldots,l\}:0<y_i\alpha_i<C \right\}\\
  \label{bounded-sv-set}
  g &= \left\{i\in\{1,\ldots,l\}: y_i\alpha_i=C \right\}\\
  n &= \left\{i\in\{1,\ldots,l\}: \alpha_i=0 \right\}.
\end{align}
Mediante interpretación geométrica de la solución al problema de la
SVM, se sabe que para aquellos vectores $\xx_i$ que no son de soporte,
se cumple que $\alpha_i=0$, luego las derivadas $\alpha_n$ respecto de los
hiperparámetros son nulas. Para el caso $\alpha_i=\pm{}C$, se tiene

\begin{align}
  \dpar{\alpha_g}{C}{} = y_g, && \dpar{\alpha_g}{{\theta}^K_j}{} = 0,
\end{align}
ya que el valor de $\alpha_g$ viene limitado (en valor absoluto) por el
parámetro $C$. Para simplificar el cálculo de las derivadas de
$\alpha_u$ y $b$ se plantea un problema en forma matricial. Sea la
\emph{matriz del núcleo} $K$

\begin{align}
  K = \begin{pmatrix} k(x_1,x_1) & k(x_1,x_2) & \cdots & k(x_1,x_l)
    \\ k(x_2,x_1) & k(x_2,x_2) & \cdots & k(x_2,x_l) \\ \vdots &
    \vdots & \ddots & \vdots \\ k(x_l,x_1) & k(x_l,x_2) & \cdots &
    k(x_l,x_l)
  \end{pmatrix}
  =
  \begin{pmatrix}
    (K_{\uu\uu}) & (K_{\uu\Bg}) & (K_{\uu\nn}) \\
    (K_{\Bg\uu}) & (K_{\Bg\Bg}) & (K_{\Bg\nn}) \\
    (K_{\nn\uu}) & (K_{\nn\Bg}) & (K_{\nn\nn})
  \end{pmatrix}
\end{align}
donde en la matriz de la derecha los elementos de $K$ han sido
reordenados en submatrices $K_{uu},K_{ug},\ldots$ según los conjuntos
de índices $u, g, n$ definidos anteriormente.  Sea la matriz

\begin{align}
  H=\begin{pmatrix} K_{uu} & \B{1}_u \\ \B{1}_u^T & 0
  \end{pmatrix}
\end{align}
con $\B{1}_u$ un vector columna de $|u|$ elementos iguales a 1,
entonces la derivada de $(\B{\alpha}_u,b)^T$ respecto del
hiperparámetro $j$ del núcleo $\theta^K_j$ viene dada por

\begin{align}
  \dpar{}{\theta^K_j}{} \begin{pmatrix}\alpha_{u}\\b\end{pmatrix} &=
    -H^{-1} \left[
      \dpar{H}{\theta^K_j}{}
      \begin{pmatrix}\alpha_{u}\\b\end{pmatrix}
        +C \begin{pmatrix}\dpar{K_{gu}}{\theta^K_j}{}\\0\end{pmatrix}
          y_g
          \right], \\
  \dpar{}{C}{} \begin{pmatrix}\alpha_{u}\\b\end{pmatrix} &=
    -H^{-1} \left[
      \begin{pmatrix}{K_{gu}}\\\B{1_g^T}\end{pmatrix} y_g
      \right],
\end{align}
donde $\B{1}_g$ un vector columna de $|g|$ elementos iguales a 1 y
$\B{y}_g$ el vector de clases correspondientes a los elementos en
$g$. Para los detalles de este resultado se refiere al lector a
\cite{glasmachers} y \cite{keerthi}.

Una vez obtenidas las derivadas de $({\B{\alpha},b})^T$ respecto de
los hiperparámetros $\B{\theta}$, la derivada de $f_k$  respecto
de los mismos viene dada por

\begin{align}
  \dpar{f_k}{C}{} &=  \sum_{i=1}^l y_i\left[
    \dpar{\alpha_i}{C}{} k(x_i,x_k) \right]
  + \dpar{b}{C}{} ,\\
  \dpar{f_k}{\theta^K_j}{} &=  \sum_{i=1}^l y_i \left[
    \dpar{\alpha_i}{\theta^K_j}{} k(x_i,x_k) +
    \dpar{k(x_i,x_k)}{\theta^K_j}{} \alpha_i \right]
  + \dpar{b}{\theta^K_j}{}. 
\end{align}
Con este resultado, resulta posible el cálculo de la derivada de la
probabilidad estimada $\dpar{\hat{p}_k}{\theta_j}{}$
(\autoref{eq:deriv-px-thetak}), y con ella la del gradiente
$\nabla{}E$ (\autoref{gradE}).

\subsubsection{Algoritmo de optimización}
Dado el conjunto de entrenamiento $D$, en cada punto de evaluación
$\Btheta$ el error empírico $E(\Btheta)$ se calcula del siguiente modo

\begin{enumerate}
\item Aplicando validación cruzada, entrenar un modelo SVM para cada
  conjunto de estimación.
\item Calcular las salidas $f_k$ (\autoref{fk}) de los
  modelos para todos los ejemplos $\xx_k$ en las respectivas
  particiones de validación.
\item Encontrar valores óptimos de los parámetros $A$ y $B$, partiendo
  de valores iniciales $A_0=1$, $B_0=0$ (\autoref{abproblem}).
\item Calcular las probabilidades estimadas $\hat{p}_k$ y con ellas,
  el error empírico de cada ejemplo $E_k$ y global $E$.
\item Calcular el gradiente $\nabla{}E$.
\end{enumerate}
Partiendo de un punto inicial $\Btheta^0$, la búsqueda procede en cada
punto $\Btheta^i$ evaluando la función objetivo y determinando un
nuevo punto $\Btheta^{i+1}$ a partir del valor $E^i=E(\Btheta^i)$ y la
información disponible en el gradiente $\nabla{}E^i$ hasta satisfacer
algún criterio de corte, por ejemplo

\begin{align*}
  |E^i-E^{i-1}|&<\eta, & \|\nabla E^i\| < \nu,
\end{align*}
donde $\eta$ y $\nu$ son números pequeños.  Este esquema es
implementado en la estrategia de minimización del error empírico
mediante el algoritmo de optimización BFGS \cite{bfgs}.
