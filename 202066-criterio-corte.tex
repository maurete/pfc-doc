%
\subsubsection{Criterio de corte}
%
Si bien no existe un criterio de corte universal para detener el
entrenamiento mediante retropropagación, pueden postularse algunos
criterios razonables para establecer la convergencia:
%
\begin{itemize}
\item Cuando la norma del gradiente es muy pequeña
  ($\|\nabla\C{E}\|\leq\epsilon$), probablemente se deba a que se
  alcanzó un mínimo local de la función de error.
\item Cuando la función de energía promedio $\C{E}_{\T{av}}$ varía muy
  poco entre épocas, también podría significar que se alcanzó un
  mínimo local, indicando convergencia.
\end{itemize}
%
El problema principal de estos criterios es que los valores de
tolerancia a considerar serán siempre dependientes del problema, y una
elección incorrecta puede llevar a un corte prematuro del
entrenamiento, o bien a que el algoritmo nunca converja.

Un criterio de convergencia alternativo y ampliamente difundido se
basa en estimar el error de generalización de la red luego de cada
iteración y abortar el entrenamiento cuando se detecta que se ha
alcanzado el mínimo.
La estimación del error de generalización se efectúa aplicando el
método de retención sobre el conjunto de entrenamiento $D$, entrenando
con un subconjunto de estimación $E$ y evaluando la red con un
subconjunto de monitoreo $M$.
