%
\subsubsection{Sesgo y varianza}
%
Los conceptos de sesgo y varianza brindan un marco de análisis para
los algoritmos de aprendizaje automático.
Desde esta perspectiva, se
identifican tres fuentes de error de generalización del modelo: el
sesgo, la varianza, y el error intrínseco, irreducible, debido a la
incertidumbre inherente a los datos.
%
\begin{itemize}
\item El sesgo hace referencia al error cometido por el modelo sobre
  el conjunto de entrenamiento. Un bajo sesgo significa que el modelo
  ``ajusta bien'' al conjunto de entrenamiento.
\item La varianza se relaciona con la complejidad del modelo. Cuando
  el modelo se torna más complejo, se vuelve susceptible a pequeñas
  variaciones en los datos de entrada, perdiendo generalidad en la
  salida.
\end{itemize}
%
El objetivo del entrenamiento es ajustar suficientemente el modelo
(bajar el sesgo) cuidando de no aumentar demasiado su complejidad
(aumentando la varianza).  Este problema se denomina frecuentemente
``el dilema sesgo--varianza''.

%% Un modelo subajustado tendrá un alto sesgo, mientras
%% que un modelo sobreajustado tendrá una alta varianza.

%% Las medidas de sesgo y varianza son útiles para los modeladores en
%% tanto que ayudan a regular la complejidad del modelo final.

%% Estas medidas se relacionan con la capacidad de ajuste y
%% generalización de un modelo.

%% Cuando se logra un gran ajuste, la diferencia entre los datos reales y
%% la estimación del modelo es pequeña, en este caso el sesgo también es
%% pequeño, pero estos buenos resultados de ajuste, van de la mano con
%% el aumento en la complejidad del modelo, cuando se aumenta la
%% complejidad del modelo este se vuelve sensible a pequeñas variaciones
%% en los datos de entrada, fluctuando en función de estos, es así cuan-
%% do la varianza aumenta.

%% Esta claro que en Aprendizaje de Maquina se busca crear modelos que
%% ofrezcan dos características esenciales: ajuste a los datos y
%% generalización.

%% Esto acentúa la necesidad de encontrar un balance entre sesgo y
%% varianza, o visto de otra forma, entre error y complejidad, a
%% continuación se describen algunos conceptos importantes referentes al
%% tema.

%% %
%% \subsubsection{El <<dilema sesgo--varianza>>}
%% %
%% El \e{dilema sesgo--varianza} hace referencia al problema de minimizar
%% simultáneamente dos fuentes de error que impiden que la máquina de
%% aprenzizaje generalizar más allá del conjunto de entrenamiento:
%% %
%% \begin{itemize}
%% \item El sesgo es el error producido por supuestos erróneos en el
%%   algoritmo de aprendizaje.  Un sesgo elevado deriva en el algoritmo
%%   siendo incapaz de identificar las relaciones importantes entre las
%%   entradas y las salidas (subajuste).
%% \item La varianza es el error producido por la sensibilidad a pequeñas
%%   fluctuaciones en el conjunto de entrenamiento. Una alta varianza
%%   puede provocar sobreajuste.
%% \end{itemize}
%% %

%% The bias–variance decomposition is a way of analyzing a learning
%% algorithm's expected generalization error with respect to a particular
%% problem as a sum of three terms, the bias, variance, and a quantity
%% called the irreducible error, resulting from noise in the problem
%% itself.

%% This tradeoff applies to all forms of supervised learning:
%% classification, regression (function fitting),[1][2] and structured
%% output learning. It has also been invoked to explain the effectiveness
%% of heuristics in human learning.
