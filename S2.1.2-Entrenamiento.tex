%
%
\subsection{Entrenamiento}
%
Todo el conocimiento que se tiene sobre la distribución generadora de
datos $\nu$ proviene del conjunto de datos finito
$D=\left((x_1,y_1),\ldots,(x_\ell,y_\ell)\right)$.  Este conjunto de
datos define una distribución empírica
%
\begin{align}
  \hat{\nu}=\frac{1}{\ell}\sum_{i=1}^{\ell}\delta_{(x_i,y_i)}(x,y)
\end{align}
%
donde $\delta_{(x,y)}$ denota el delta de Dirac en
$(x,y)\in{}X\times{}Y$.  Esta distribución conduce a la definición del
\e{error de entrenamiento} como el error de un modelo $h$ sobre los
datos de entrenamiento en $D$:
%
\begin{align}
  \hat{\C{R}}_D(h)=E_\T{entrenamiento}=\R{E}_{\hat{\nu}}[L(x,y,h(x))]
  =\frac{1}{\ell}\sum_{i=1}^\ell L(x_i, y_i, h(x_i)),
\end{align}
%
en donde $L$ es una función de pérdida.

El entrenamiento de la máquina de aprendizaje consiste en
seleccionar el modelo con el mínimo error de entrenamiento
%
\begin{align}
  \hat{h}:=\arg \min \left\{ \hat{\C{R}}_D(h)\middle|h \in H\right\}.
\end{align}
%
Este procedimiento se conoce también como la minimización del riesgo
empírico, y define una máquina de aprendizaje
$A_H:\C{D}\rightarrow{}H$ para cada clase $H$ de modelo.
