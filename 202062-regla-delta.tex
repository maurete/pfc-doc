%
\subsubsection{Ajuste de los pesos de la red}
%
En el caso del entrenamiento en línea, en cada paso del entrenamiento
$t$ se calcula la respuesta de la red
$\B{s}(t)$ para la entrada $\xx(t)$ y luego se calcula el gradiente
$\dpar{\C{E}(t)}{w_{ij}}{}$ propagando hacia atrás la señal de error $\C{E}(t)$.
A partir de la información del gradiente
se aplica una corrección $\Delta{}w_{ij}(t)$ sobre el peso
$w_{ij}(t)$, según la ``regla delta''
%
\begin{align}
\label{e2:delta-rule}
  \Delta w_{ij}(t)\tab=-\eta\dpar{\C{E}(t)}{w_{ij}}{},
\end{align}
%
en donde $\eta$ es el parámetro \e{velocidad de aprendizaje} del
algoritmo de retropropagación. La utilización del signo menos en la
\iflatexml{}Ecuación~\ref{e2:deriv-E-wrt-w}
\else\autoref{e2:deriv-E-wrt-w}\fi{} indica el \e{descenso por
  gradiente} en el espacio de los pesos $w_{ij}$. Esto implica que el
ajuste efectuado modifica los pesos en la dirección que reduce el
valor de $\C{E}(t)$.
