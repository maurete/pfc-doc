%
\subsubsection{Ajuste de los pesos $\ww_{ij}$ en el entrenamiento por época}
%
En cada época $t$ se propagan hacia adelante todos los vectores
$\xx(n)$ del conjunto de entrenamiento $D$, y a partir de los valores
de error instantáneo $\C{E}(n)$ se calcula la energía de error
promedio $\C{E}_{\T{av}}(t)$ según (\ref{e2:average-energy-net}).
El gradiente de error promedio para la época viene dado por
%
\begin{align}\label{e2:deriv-Eav-wrt-w}
  \dpar{\C{E}_{\T{av}}(t)}{w_{ij}}{}\tab=
  \frac{1}{\ell}\sum_{n=1}^\ell\dpar{\C{E}(n)}{w_{ij}}{},
\end{align}
%
y la corrección a aplicar a los pesos viene dada por
%
\begin{align}\label{e2:delta-rule-epoch}
  \Delta w_{ij}(t)\tab=-\eta\dpar{\C{E}_{\T{av}}(t)}{w_{ij}}{}=
  -\frac{\eta}{\ell}\sum_{n=1}^\ell\dpar{\C{E}(n)}{w_{ij}}{}.
\end{align}
%
Dado que la función $\C{E}_{\T{av}}$ se calcula a partir de todos los
ejemplos del conjunto $D$, la superficie de la función de error
resultante es más ``suave'' y el gradiente $\nabla\C{E}_{\T{av}}$ es
numéricamente más estable que en el entrenamiento en línea, lo que
garantiza la convergencia a un mínimo local.
