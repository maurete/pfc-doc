
\subsection{Minimización de la cota radio-margen}
La cota ``radio-margen'' es una función con la propiedad de ser un
límite superior al error de validación cruzada dejando uno fuera, y se
calcula directamente a partir de la información disponible en el
modelo de una máquina de vectores de soporte.  La función
``radio-margen'' propuesta originalmente por \citeauthor{vapnik}
\cite{vapnik} para un modelo SVM  con término de regularización $L2$
(\autoref{svm-l2}), viene dada por % vapnik sec. 10.7, pag 441

\begin{align}
  \T{RM} = 4R^2 \|w\|^2.
\end{align}
Aquí, $R$ es el radio de la hiperesfera que, en el espacio inducido
por el núcleo, engloba todos los vectores de soporte, y $\|\ww\|$ es
la norma del vector normal al hiperplano de separación, que define el
margen del modelo $h$ de la SVM.  Para esta cota, se cumple
desigualdad

\begin{align}
  E_{\T{LOO}} \leq \T{RM},
\end{align}
donde $E_{\T{LOO}}$ es el error de validación cruzada dejando uno
fuera.

El error de validación cruzada dejando uno fuera $E_{\T{LOO}}$ es uno
de los mejores estimadores del error de generalización disponibles.
Por ello, la idea de minimizar una cota de este error, que es a la vez
es continua, derivable y fácilmente calculable, resulta muy atractiva.
De hecho, en \cite{chapelle} se presenta un algoritmo de minimización
de la cota $\T{RM}$ mediante descenso por gradiente en el espacio de
los hiperparámetros.  Sin embargo, dado que la mayoría de SVMs
utilizadas en la práctica incorporan regularización $L1$ en lugar de
$L2$, la utilidad real de minimizar la cota $\T{RM}$ es limitada.

En \cite{chung} se proponen varias funciones ``alternativas'',
inspiradas en la cota radio-margen $\T{RM}$, y aplicables al caso
específico del núcleo gaussiano (RBF) y formulación $L1$. La
estrategia aquí propuesta utiliza una de estas cotas
alternativas, denotada $\rho$ y definida según

\begin{align}
  \rho = \rho_R \cdot \rho_M,
\end{align}
donde $\rho_R$, $\rho_M$ son los factores ``radio'' y ``margen''

\begin{align}
  \rho_R &= R^2+\frac{1}{C}, \\
  \rho_M &= \|\ww\|^2+2C\sum\xi_i.
\end{align}
A diferencia de $\T{RM}$, la función $\rho$ ajusta el valor
real de $E_{\T{LOO}}$ con demasiada holgura, con lo que su valor
pierde el significado original de representar una tasa de error. Sin
embargo, tiene la importante propiedad de poseer un mínimo global en
las cercanías de los hiperparámetros óptimos $(C,\gamma)$ que
minimizan la tasa de error $E_{\T{LOO}}$.

\subsubsection{Cálculo de $\rho$}
El valor $R^2$, necesario para el cálculo de $\rho_R$, viene dado por

\begin{align}
  R^2 = 1 - \Bbeta_*^T \KK \Bbeta_*,
\end{align}
donde $\Bbeta_*$ es la solución al problema de optimización conocido
como ``SVM de una clase'' \cite{scholkopf}

\begin{align}
\begin{split}
  \arg\min_{\Bbeta}\quad&\Bbeta^T \KK \Bbeta,\\
  \T{sujeto a}    \quad&0\leq\beta_i\leq{}1,\quad{}i=1,\ldots,\ell,\\
                       &\B{1}^T_\ell\,\Bbeta=1.
  \end{split}
  \label{svm-oneclass}
\end{align}
Aquí, $\B{1}_{\ell}$ es un vector columna con $\ell$ elementos iguales
a 1, y $\KK$ es la matriz del núcleo con elementos
$k_{ij}=k(\xx_i,\xx_j)$. La matriz $\KK$ es definida positiva siempre
que se cumpla la condición

\begin{align}
\label{cond-kmatrix-defpos}
  i\neq j \iff \xx_i\neq\xx_j,\quad \forall\,i,j\in{1,\ldots,\ell},
\end{align}
esto es, siempre que no haya ejemplos repetidos en el conjunto de
entrenamiento. Sin perder mucha generalidad, se considera que tal es
el caso, y con ello, se asegura que el problema \cite{svm-oneclass}
tiene solución única \cite{SOL-UNICA-DEFINIDA-POSITIVA}.

El valor ``margen'' $\rho_M$ es dos veces la solución al problema de
optimización de la SVM (\autoref{svm-primal-blando}).  Dada la
equivalencia primal-dual de la solución, si $\Balpha$ es solución a
la forma dual (\autoref{svmprob-dual-soft}), se tiene simplemente

\begin{align}
\label{prieqdual}
  \rho_M &= 2\left(\frac{1}{2}\|\ww\|^2+C\sum\xi_i \right)
  =2\left(\B{e}^T\Balpha-\frac{1}{2}\Balpha^T\B{Q}\Balpha\right),
\end{align}
donde $\QQ$ es la matriz con elementos $Q_{ij}=y_iy_jk(\xx_i,\xx_j)$.

\subsubsection{Cáclulo del gradiente $\nabla\rho$}
El cálculo de las derivadas de $\rho$ se basa en resultados de
análisis de perturbación de problemas de optimización, ya que tanto
$\rho_R$ como $\rho_M$ son soluciones a problemas de este tipo.  La
discusión en este apartado se basa en aquella presentada por
\citeauthor{chung} en \cite{chung}.

El Teorema 4.1 de \cite{bonnans-shapiro} establece:

\begin{quote}
  Considérese un problema de optimización en la forma
  $f_*(\vv)=\min_{\uu\in{}U}f(\uu,\vv)$ con vector $\uu$ de variables a
  optimizar, vector de parámetros $\vv$, y $U$ independiente de
  $\vv$. Entonces, si para cada $\vv$ existe un $\uu_*(\vv)\in{}U$
  tal que $f_*(\vv)=f(\uu_*(\vv),\vv)$, la derivada
  $\dpar{f_*}{\vv}{}(\vv)$ existe y $\dpar{f_*}{v_i}{}$ viene dado por
  $\dpar{f_*}{v_i}{}(\uu_*,\vv)$ para cualquier parámetro $v_i$.
\end{quote}

Como se ha visto anteriormente, exigiendo la no-repetición de ejemplos
en el conjunto de entrenamiento (\ref{cond-kmatrix-defpos}) se
garantiza la existencia de la solución $\uu_*$.
El problema (\ref{svm-oneclass}) cumple con los requisitos del teorema,
luego las derivadas de $R^2$ respecto de los hiperparámetros vienen
dadas por

\begin{align}
  \dpar{R^2}{C}{} &= \dpar{(1-\Bbeta^T\KK\Bbeta)}{C}{}
  = -\Bbeta^T \dpar{\KK}{C}{}\Bbeta = 0, \\
  \dpar{R^2}{\gamma}{} &= \dpar{(1-\Bbeta^T\KK\Bbeta)}{\gamma}{}
  = -\Bbeta^T \dpar{\KK}{\gamma}{}\Bbeta.
\end{align}
En el caso $\rho_M$ el teorema anterior no aplica, ya que

\begin{enumerate}
\item La restricción $\alpha_i\leq{}C$ depende de $C$, violando la condición
  necesaria ``$U$ independiente de $\vv$''.
\item El vector $\Bxi$ óptimo puede no ser único ya que el parámetro $b$ óptimo
  puede no ser único cuando el modelo no contiene vectores de soporte
  ``libres'' (esto es, cuando $u=\emptyset$ (\ref{unbounded-sv-set})).
\end{enumerate}
Para salvar estos inconvenientes, se aprovecha la dualidad del
problema en el punto óptimo $\Balpha_*$, y se efectúa el cambio de
variable $\bar{\Balpha}=\Balpha/C$ en el problema dual
(\ref{svmprob-dual-soft}) de modo que las restricciones sean
independientes del hiperparámetro $C$

\begin{align}
\begin{split}
    \max_{\bar{\Balpha}}\quad&
    f(\bar{\Balpha}) = C^2 \left( \frac{\B{1}^T\bar{\Balpha}}{C}
    -\frac{1}{2}\bar{\Balpha}^T\QQ\bar{\Balpha}\right)\\
    \T{sujeto a}\quad & \yy^T\bar{\Balpha} = 0, \\
    & 0\leq\bar{\alpha}_i\leq 1,
    \T{ para todo } i\in {1,\ldots,\ell }.
\end{split}\end{align}
Este problema cumple con las condiciones del teorema ya que
no depende de $\Bxi$ y cuenta con solución única
ya que $Q$ es definida positiva (\ref{cond-kmatrix-defpos}). Entonces,

\begin{align}
\label{eq:rmb-alpha-equiv}
  \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha
  &= C^2\left(\frac{\B{1}^T\bar{\Balpha}}{C} -
  \frac{1}{2}\bar{\Balpha}^T\QQ\bar{\Balpha}\right).
\end{align}
Con estos resultados se está en condiciones de calcular las derivadas de
$\rho$ respecto de los hiperparámetros $C$ y $\gamma$:
Las derivadas respecto a $C$ vienen dadas por

\begin{align}
  \begin{aligned}
    \dpar{\rho_M}{C}{}
    &= \dpar{}{C}{}2\left(  \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha \right) \\
    &= \dpar{}{C}{}\left( 2C^2\left(\frac{\B{1}^T\bar{\Balpha}}{C} -
    \frac{1}{2}\bar{\Balpha}^T\QQ\bar{\Balpha}\right)
    \right) \\
    &= 4C \left(\frac{\B{1}^T\bar{\Balpha}}{C} -
    \frac{1}{2}\bar{\Balpha}^T\QQ\bar{\Balpha}\right) - 2C^2 \left(\frac{\B{1}^T\bar{\Balpha}}{C^2} \right) \\
    &= 2\left(\B{1}^T\bar{\Balpha} - C \bar{\Balpha}^T\QQ\bar{\Balpha}\right) \\
    &= \frac{2}{C} \left(\B{1}^T\Balpha - \Balpha^T\QQ\Balpha\right)
  \end{aligned}
  \\
  \begin{aligned}
    \dpar{\rho_M}{C}{}
    &= \dpar{}{C}{} \left( R^2 + \frac{1}{C} \right) \\
    &= -\frac{1}{C^2}.
  \end{aligned}
\end{align}

Las derivadas respecto del hiperparámetro $\gamma$ vienen dadas por

\begin{align}
  \begin{aligned}
    \dpar{\rho_M}{\gamma}{}
    &= \dpar{}{\gamma}{} 2\left(  \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha \right) \\
    &= - \Balpha^T \dpar{\QQ}{\gamma}{}\Balpha \\
    & = - \Balpha^T \left(\yy^T \dpar{\KK}{\gamma}{}\yy\right) \Balpha
    %% \\
    %% & = \sum_{i,j=1}^\ell \alpha_i\alpha_j y_i y_j \dpar{k(\xx_i,\xx_j)}{\gamma}{}, \\[0.2em]
  \end{aligned}
  \begin{aligned}
    \dpar{\rho_R}{\gamma}{} &= \dpar{R^2}{\gamma}{} \\
    = - \Bbeta^T \dpar{\KK}{\gamma}{} \Bbeta. % \\
%    &= \sum_{i,j=1}^\ell \beta_i\beta_j \dpar{k(\xx_i,\xx_j)}{\gamma}{}.
  \end{aligned}
\end{align}
Los elementos $\dpar{k_{ij}}{\gamma}{}$ de la matriz $\dpar{\KK}{\gamma}{}$ 
vienen ddados por

\begin{align}
  \dpar{k_{ij}}{\gamma}{}
  = \dpar{}{\gamma}{}k(\xx_i,\xx_j)
  = \dpar{}{\gamma}{} \left(e^{-\gamma\|\xx_i-\xx_j\|}\right)
  = -k_{ij}\|\xx_i-\xx_j\|
\end{align}
Con todo esto, las derivadas de la función $\rho$ respecto de los
hiperparámetros vienen dadas por

\begin{align*}
    \dpar{\rho}{C}{} &= \dpar{\rho_M}{C}{} \rho_R + \rho_M \dpar{\rho_R}{C}{} \\
    &= \frac{2}{C} \left(\B{1}^T\Balpha - \Balpha^T\QQ\Balpha\right) \left( R^2 + \frac{1}{C} \right)
    - 2\left(  \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha \right)
    \left( \frac{1}{C^2} \right)\\
    &=
  \\[2em]
    \dpar{\rho}{\gamma}{} &= \dpar{\rho_M}{\gamma}{} \rho_R + \rho_M \dpar{\rho_R}{\gamma}{}\\
    &= \left( - \Balpha^T \left(\yy^T \dpar{\KK}{\gamma}{}\yy\right) \Balpha \right)
    \left( R^2 + \frac{1}{C} \right)
    - 2\left(  \B{1}^T\Balpha-\frac{1}{2}\Balpha^T\QQ\Balpha \right)
    \left( \Bbeta^T \dpar{\KK}{\gamma}{} \Bbeta \right)
\end{align*}
donde

\begin{align}
  k_{ij}&=k(x_i,x_j)=e^{-\gamma||x_i-x_j||},\,\, \T{y} \\
  \dpar{k_{ij}}{\gamma}{}&=-k_{ij}||x_i-x_j||
\end{align}
son la función kernel RBF y su derivada respecto del hiperparámetro $\gamma$,
respectivamente.

\subsubsection{Algoritmo de optimización}
Partiendo de un punto inicial $(C^0,\gamma^0)$ la búsqueda procede
en cada punto $(C^k,\gamma^k)$ evaluando $\rho^k$ y determinando
un nuevo punto $(C^{k+1},\gamma^{k+1})$ en la dirección del
negativo del gradiente $\nabla\rho^k$ tal que $\rho^{k+1}<\rho^k$
La optimización de la función $\rho$ se efectúa mediante el algoritmo
BFGS \cite{bfgs} en el espacio logarítmico de los hiperparámetros
$(\ln(C),\ln(\gamma))$. Este cambio de coordenadas se traduce en un
incremento de la estabilidad numérica y evita tener que verificar en
cada iteración la no-negatividad de $C$ y $\gamma$.

\begin{align}
  \dpar{\rho}{\ln C}{}= C \dpar{\rho}{C}{}, &&
  \dpar{\rho}{\ln \gamma}{}= \gamma \dpar{\rho}{\gamma}{}
\end{align}
Para un conjunto de entrenamiento $D=((\xx_i,y_i),i=1,\ldots,\ell)$
cada evaluación de $\rho(\Btheta_k)$ consta de los siguientes pasos

\begin{enumerate}
\item Entrenar una máquina de vectores de soporte con núcleo RBF
  con hiperparámetros $\Btheta_k=(C_k,\gamma_k)$ sobre el conjunto
  de entrenamiento completo $D$
\item Calcular $\Bbeta_*$ óptimo para el problema (\ref{svm-oneclass})
\item Calcular el valor de $\rho$ como el producto de
  $\rho_M$ (\ref{rho_m}) y $\rho_r$ según (\ref{rho_r})
\item Calcular el gradiente $\nabla\rho$.
\end{enumerate}